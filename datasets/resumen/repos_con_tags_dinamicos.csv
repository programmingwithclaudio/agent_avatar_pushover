repo_nombre,es_privado,tiene_readme,documentacion,archivo_origen,fecha_actualizacion,url_repositorio,clasificacion_dinamica
checkpoint-DS-org/CP05_20FT_01,Sí,Sí,"Carrera Data Science - Módulo 1 Bienvenido al Checkpoint del Módulo 1 de la Carrera Data Science. ¡ Por favor lee TODO este material con atención INTRODUCCION El Checkpoint es un desafío técnico donde evaluamos conceptos vistos en el módulo. En esta instancia, buscamos asegurarnos que todos nuestros aplicantes tengan una base de conocimientos mínimos necesarios para luego seguir aprendiendo temas nuevos. Debe resolverse de manera individual. Si te copias o recibes ayuda de compañeros, además de estar incumpliendo con las normas de Henry (lo que te dejaría afuera de la carrera), estarás perjudicándote a ti mismo, porque el primer día de clase estarías extremadamente perdido. PASOS PARA RESOLVER EL CHECKPOINT: FORK Primero debes forkear este repo, haciendo click en el botón de arriba a la derecha. Una vez que tengas una copia de este repo en tu cuenta de , cloná el repo dentro de una nueva carpeta (en nuestro ejemplo, va a ser ""checkpoint_m1""). Asegurate de no utilizar la misma que el prep curse. Una vez clonado entrá a esa carpeta y ejecutá los siguientes comandos: python tests.py Si ves los tests fallando, estás listo para comenzar, si no lee bien el output para identificar cual es el error. RESOLVER EL CHECKPOINT Tu tarea es completar el código en de tal forma que pasen la mayoría de los tests. ENTREGAR TU CHECKPOINT Correr por ultima vez los tests y verificar cuantos pasan. Ten en cuenta que si te aparece ""1 failed;1 total"" es porque tienes un error de sintaxis: seguramente falta o sobra un corchete, paréntesis, dos puntos, etc. Saca un print de pantalla de tus tests. Luego, debes subir un commit a tu repo. Para hacerlo, debes ejecutar el siguiente comando: git add . git commit -m 'checkpoint commit' git push origin main Una vez finalizado, chequea: Que veas los cambios reflejados en el repo de tu cuenta de github (entrando a tu repo desde el browser.) Que no haya un require - solo debe haber codigo dentro de las funciones de cada ejercicio Atención: no debes realizar un commit después de la hora de entrega porque se anulara la totalidad del examen. Revisar la hora del entrega del examen en los emails que te llegaron. ¿TENES ALGUN PROBLEMA / CONSULTA? Si te ocurre algún problema, revisa los canales en Slack. Probablemente a algún compañero le paso algo similar y ya lo consulto. Si no encuentras la respuesta, puedes publicar un mensaje en dicho canal. No se puede hacer consultas sobre la resolucion de los ejercicios. GUIA DE ERRORES COMUNES Para...",README.md,2023-12-01 12:42:07,https://github.com/checkpoint-DS-org/CP05_20FT_01,"{""proposito_principal"": ""Evaluación técnica de conocimientos básicos de Data Science para estudiantes"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Evaluación técnica"", ""Ejercicios de programación""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Ejecución de tests automatizados"", ""Evaluación de código Python""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Educativo"", ""Evaluación"", ""Checkpoint"", ""Carrera Data Science""]}"
diip-io/diip-io,Sí,Sí,diip-io,README.md,2024-05-07 06:51:21,https://github.com/diip-io/diip-io,"{""proposito_principal"": ""No se puede determinar - falta información específica sobre el propósito del proyecto"", ""dominio_aplicacion"": ""No se puede determinar - falta información sobre el dominio de aplicación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
diip-io/nextjs-tailwindcss-blog-,No,No,Sin documentación disponible,,2024-05-10 01:23:54,https://github.com/diip-io/nextjs-tailwindcss-blog-,"{""proposito_principal"": ""Blog personal o corporativo con Next.js y Tailwind CSS"", ""dominio_aplicacion"": ""Blogging/Publicación de contenido"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js"", ""Tailwind CSS""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Template"", ""Blog""]}"
HerlinData/ai_car_company,Sí,Sí,"aicarcompany Automatizacion git checkout = cambiar de rama git pull origin = traer cambios de una rama git merge = unir cambios de ambas ramas git merge, no siempre se usa, cuando hay info personal de avance de un colaborador crear entorno virtual = python -m venv env activar entorno virtual, siempre que se quiere trabajar = env\Scripts\activate instalar todas las librerias = pip install -r requirements.txt actualizar siempre python ejecutar un archivo python cmd = python archive_name.py",README.md,2023-12-15 19:55:40,https://github.com/HerlinData/ai_car_company,"{""proposito_principal"": ""Automatización de procesos de desarrollo y gestión de versiones para un proyecto de IA"", ""dominio_aplicacion"": ""Desarrollo de Software"", ""tipo_proyecto"": [""Herramienta de Automatización"", ""Scripting""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Git""], ""funcionalidades_clave"": [""Gestión de ramas Git"", ""Creación de entornos virtuales"", ""Instalación de dependencias"", ""Ejecución de scripts Python""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Documentación de procesos"", ""Guías de desarrollo""]}"
HerlinData/pet_store_api,Sí,Sí,Project Pet Store update databases lol chat gpt,README.md,2023-12-13 16:36:48,https://github.com/HerlinData/pet_store_api,"{""proposito_principal"": ""API para gestión de tienda de mascotas"", ""dominio_aplicacion"": ""E-commerce"", ""tipo_proyecto"": [""API REST""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/advanced-python,Sí,Sí,"El Zen PEPs Python Enhancement Proposal (Propuesta de mejora Python) PEP rv8 realpython pep 8 Máxima cantidad de líneas para un 79 SnakeCase, nomenclatura en python gitignore io unipython ahorcado pre commit GitHub flake 8 mockaroo pokeapi google fonts Expresiones regulares List Comprehension Repo project Run app-linkedin prefect and tasks César Mayta w3techs ¿Cuál es la ventaja de la concurrencia frente al paralelismo? Rpta.: La concurrencia no necesita consumir más recursos del servidor ¿Comó se implementa la concurrencia en Python? Rpta.: Usando el paquete theading que viene en el core de python ¿Qué provoca el racecondition? Que el resultado no sea el esperado por alteraciones en una variable ¿Cuándo se produce un Deadlock? Cuando un grupo de threads se esperan unas a otras y ninguna puede continuar. ¿Qué permite el gil de python? Permite que un solo thread se ejecute a la vez en el mismo proceso. Async, Callbacks, futures, colas y semáforos. ¿Cómo se puede implementar callbacks en python? Rpta. : Creando funciones que son llamadas desde otras funciones ¿A qué se le conoce como futuros en Python? Rpta.: Con el paquete asyncio que viene por defecto en python. ¿Comó se implementa colas en python? Rpta.: Con el módulo queue ¿Comó se implementa semáforos en python? Rpta.: Con el módulo semaphore. ¿Qué es un etl? Extract transform load. ¿Qué paquete nos ayuda mejor a organizar nuestras tareas? Rpta. : Prefect ¿Cuándo debemos crear tareas asincronas en un etl? Rpta.: Para optimizar el tiempo de extración de datos, cuando la data al extraer crece y el tiempo de respuesta es más largo, cuando deben hacer múltiples extracciones y cargas al mismo tiempo, se quiere optimizar el tiempo de carga de datos. ¿Comó hacemos que una tarea se ejecute periódicamente? Rpta.: Usando Cron Jobs en un servidor de Linux. O Jenkins Web Scraping Manejador de dependencias en Pythcon: https://python-poetry.org Herramientas para web scraping usadas: BeautifulSoup https://www.crummy.com/software/BeautifulSoup Selenium https://selenium-python.readthedocs.io Puppeteer https://pptr.dev Link para descargar web driver de google chrome: https://googlechromelabs.github.io/chrome-for-testing Repositorio con código fuente (Python) https://github.com/pabloespana/code-web-scraping-python Repositorio con código fuente (Node.js) https://github.com/pabloespana/code-web-scraping-nodejs",README.md,2024-08-25 22:10:23,https://github.com/programmingwithclaudio/advanced-python,"{""proposito_principal"": ""Proyecto educativo avanzado de Python que cubre múltiples conceptos y técnicas de programación"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Educational Project"", ""Code Examples"", ""Tutorial Repository""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""pre-commit"", ""GitHub""], ""funcionalidades_clave"": [""Concurrencia con threading"", ""Programación asíncrona con asyncio"", ""Web Scraping"", ""ETL (Extract Transform Load)"", ""Tareas programadas con Cron Jobs"", ""Manejo de colas y semáforos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Educational"", ""Best Practices"", ""PEP 8"", ""Code Quality""]}"
programmingwithclaudio/AdventureWorks-for-Postgres,No,Sí,"AdventureWorks for Postgres This project provides the scripts necessary to set up the OLTP part of the go-to database used in training classes and for sample apps on the Microsoft stack. The result is 68 tables containing HR, sales, product, and purchasing data organized across 5 schemas. It represents a fictitious bicycle parts wholesaler with a hierarchy of nearly 300 employees, 500 products, 20000 customers, and 31000 sales each having an average of 4 line items. So it's big enough to be interesting, but not unwieldy. In addition to being a well-rounded OLTP sample, it is also a good choice to demonstrate ETL into a data warehouse. Provided is a ruby file to convert CSVs available on CodePlex into a format usable by Postgres, as well as a Postgres script to create the tables, load the data, convert the hierarchyid columns, add primary and foreign keys, and create some of the views used by Adventureworks. How to set up the database: Download Adventure Works 2014 OLTP Script. Extract the .zip and copy all of the CSV files into the same folder, also containing updatecsvs.rb file and install.sql. Modify the CSVs to work with Postgres by running: Create the database and tables, import the data, and set up the views and keys with: (If you do not have a database created for your user account then you may need to also add: to the above two commands.) All 68 tables are properly set up, and 11 of the 20 views are established. The ones not built are those that rely on XML functions like value and ref. To see a list of tables, open psql, and then connect to the database and show all the tables with these two commands: Using with Docker You can spin up a new database using Docker with . You will need to rename the Adventure Works 2014 OLTP Script archive to adventureworks2014OLTPscript.zip to get this to work_ Motivation Five years ago I was pretty happy developing .NET apps for large organizations. The stack was mature, and good practices surrounding software development were very respected. The same kind of approach I appreciated from my days writing Java code was there, and the community was passionate. Then along came Windows 8. The //build/ conference in September 2011 revealed its first beta, and even with that early peek at the new direction things were headed, it was clear that everything about the platform was a haphazard combination of the new Metro apps along with all the traditional control panel and options and API for classic code. It left a very bad...",README.md,2024-01-09 16:16:07,https://github.com/programmingwithclaudio/AdventureWorks-for-Postgres,"{""proposito_principal"": ""Proporcionar scripts para configurar la base de datos OLTP AdventureWorks en PostgreSQL para entrenamiento y aplicaciones de muestra"", ""dominio_aplicacion"": ""Educación/Entrenamiento"", ""tipo_proyecto"": [""Database Migration"", ""ETL Tool"", ""Sample Database""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Conversión de CSVs a formato PostgreSQL"", ""Migración de datos desde SQL Server"", ""Creación de tablas y esquemas"", ""Configuración de claves primarias y foráneas"", ""Creación de vistas"", ""Conversión de columnas hierarchyid""], ""lenguajes_programacion"": [""Ruby"", ""SQL""], ""tags_adicionales"": [""Open Source"", ""Training Database"", ""Sample Data"", ""Database Migration""]}"
programmingwithclaudio/agent-ia-multimodal,Sí,Sí,".\venv\Scripts\Activate.ps1 # ← En PowerShell uvicorn app.main:app --reload git tag -a v1.0.1 560c9f -m ""Versión 1.0.1 - update 06-07"" git push origin v1.0.1 uvicorn app:app --port 8001 --reload git tag -a v1.0.2 88819 -m ""Versión 1.0.2- update 07-07"" git push origin v1.0.2 uvicorn app:app --port 8001 --reload",README.md,2025-07-08 05:49:47,https://github.com/programmingwithclaudio/agent-ia-multimodal,"{""proposito_principal"": ""API para agente IA multimodal"", ""dominio_aplicacion"": ""Inteligencia Artificial"", ""tipo_proyecto"": [""API REST"", ""Microservicio""], ""tecnologias_backend"": [""FastAPI""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Desarrollo con recarga automática"", ""Versionado con tags""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Producción""]}"
programmingwithclaudio/airflow_ml,Sí,Sí,"Machine Learning in production using Apache Airflow To build a solution using Machine Learning is a complex task by itself. Whilst academic Machine Learning has its roots in research from the 1980s, the practical implementation of Machine Learning Systems in production is still relatively new. This project is an example of how you can improve the two parts of any Machine Learning project - Data Validation and Model Evaluation. The goal is to share practical ideas, that you can introduce in your project relatively simple, but still achieve great benefits. Data Validation is the process of ensuring that data is present, correct, and meaningful. Ensuring the quality of your data through automated validation checks is a critical step in building data pipelines at any organization. Model validation occurs after you successfully train the model given the new data. We evaluate and validate the model before it's promoted to production. Ideally, the offline model validation step should include. Installation The project is dockerized and you have two options to run it: the Here is will be pulled from the Docker Hub; you can also build the Docker image by yourself; will initialize all necessary configs; will start up your application detached mode. After the application is started, you can easily have access to the project by the link http://localhost:8080/ Useage will create a new Bash session in the container. stops running containers without removing them. stops and removes containers. Experiments tracking from docker environment",README.md,2024-04-09 08:45:33,https://github.com/programmingwithclaudio/airflow_ml,"{""proposito_principal"": ""Implementar Machine Learning en producción con validación de datos y evaluación de modelos usando Apache Airflow"", ""dominio_aplicacion"": ""Machine Learning"", ""tipo_proyecto"": [""Pipeline de datos"", ""MLOps""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Data Validation"", ""Model Evaluation"", ""Experiments tracking"", ""Pipeline de ML en producción""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""Producción"", ""Dockerizado""]}"
programmingwithclaudio/api-controlnet-devs,Sí,Sí,"Proyecto n8n TEST (https://github.com/morhetz/gruvbox) (https://github.com/programmingwithclaudio/dotfiles) (https://opensource.org/licenses/MIT) http://localhost:9001/login Login minioadmin - minioadmin http://localhost:5678/setup Creala por defecto http://localhost:8080/ http://localhost:8080/manager Login http://localhost:8080 - 6f452646de12e76ae1625de209d77862 Utiliza el nombre del servicio en la red interna: Dado que ambos contenedores están en la misma red (), debes emplear el nombre del servicio definido en el docker-compose. En este caso, en lugar de , utiliza: Esto le indica a Evolution API que debe enviar los eventos al contenedor n8n. Verifica la configuración de los parámetros de webhook: Observa que en la configuración actual tienes: Si la intención es que Evolution API escuche o envíe eventos de forma global, es posible que necesites establecerlo en o revisar la documentación de Evolution API para confirmar si se requiere algún otro parámetro para gestionar las llamadas a webhook. Asegúrate de que la URL del servidor esté correctamente configurada: Actualmente, el parámetro en Evolution API está configurado como . Si esta URL se utiliza internamente para construir enlaces o redirigir llamadas, es posible que debas ajustarla para reflejar la red interna o la dirección externa correcta según el flujo de trabajo que estés implementando. Proyecto n8n TEST (https://postimg.cc/hQSpqXzw) Flujo de nodos e intrucciones.",README.md,2025-04-24 23:11:23,https://github.com/programmingwithclaudio/api-controlnet-devs,"{""proposito_principal"": ""Proyecto de prueba y configuración de n8n para automatización de flujos de trabajo con Evolution API"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""Workflow Automation"", ""Integration Platform""], ""tecnologias_backend"": [""n8n"", ""Evolution API""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose""], ""funcionalidades_clave"": [""Configuración de webhooks"", ""Automatización de flujos"", ""Integración entre servicios"", ""Gestión de contenedores""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Testing"", ""Configuration"", ""Integration""]}"
programmingwithclaudio/api-fastapi-login-example,Sí,Sí,Proceso Deploy Proyecto,README.md,2024-11-25 03:09:47,https://github.com/programmingwithclaudio/api-fastapi-login-example,"{""proposito_principal"": ""API de autenticación con FastAPI que incluye registro, login, verificación de email y gestión de usuarios"", ""dominio_aplicacion"": ""Desarrollo de Software"", ""tipo_proyecto"": [""API REST"", ""Template/Example""], ""tecnologias_backend"": [""FastAPI"", ""SQLAlchemy"", ""Alembic"", ""Uvicorn"", ""Pydantic"", ""Python-JOSE"", ""Passlib"", ""Python-multipart""], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose""], ""funcionalidades_clave"": [""Autenticación JWT"", ""Registro de usuarios"", ""Login con email y contraseña"", ""Verificación de email"", ""Hashing de contraseñas con bcrypt"", ""Migraciones de base de datos"", ""Documentación automática con Swagger""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Authentication"", ""Open Source"", ""Educational"", ""Template Project""]}"
programmingwithclaudio/api-flask-login-example,Sí,Sí,Objests metodos Objests metodos,README.md,2024-11-24 21:48:45,https://github.com/programmingwithclaudio/api-flask-login-example,"{""proposito_principal"": ""API de ejemplo para implementar sistema de autenticación y login con Flask"", ""dominio_aplicacion"": ""Desarrollo Web"", ""tipo_proyecto"": [""API REST""], ""tecnologias_backend"": [""Flask""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Autenticación"", ""Login de usuarios""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Ejemplo educativo"", ""Template""]}"
programmingwithclaudio/api-openwa-node,Sí,Sí,,README.md,2025-09-01 10:21:05,https://github.com/programmingwithclaudio/api-openwa-node,"{""proposito_principal"": ""No se puede determinar por falta de documentación"", ""dominio_aplicacion"": ""No se puede determinar por falta de documentación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/api-restfull-init,Sí,Sí,Cómo usar usar los recuros para para el módulo 4 Inicia el Proyecto RESTFULL FLASK flask marshmallow docker bash revisar pip post Enviar post metodo put añador decorador de la libreria flask Resource Method Decorators,README.md,2024-09-25 19:34:15,https://github.com/programmingwithclaudio/api-restfull-init,"{""proposito_principal"": ""Proyecto educativo para iniciar con API RESTful usando Flask"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""API REST"", ""Template"", ""Proyecto educativo""], ""tecnologias_backend"": [""Flask"", ""Flask-RESTful"", ""Marshmallow""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Decoradores de recursos Flask"", ""Métodos HTTP (POST, PUT)"", ""Validación de datos con Marshmallow""], ""lenguajes_programacion"": [""Python"", ""Bash""], ""tags_adicionales"": [""Open Source"", ""Educational"", ""Starter Template""]}"
programmingwithclaudio/api_ml_lendingclub,Sí,Sí,apimllendingclub,README.md,2024-01-18 02:34:07,https://github.com/programmingwithclaudio/api_ml_lendingclub,"{""proposito_principal"": ""API para análisis de riesgo crediticio usando machine learning"", ""dominio_aplicacion"": ""Finanzas"", ""tipo_proyecto"": [""API REST"", ""Machine Learning""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/api_negocio_tech,No,Sí,"Proyecto API para base de datos ""storeAPP"" Descripción Este proyecto tiene como objetivo crear una API para gestionar una base de datos normalizada para negocios utilizando el framework FastAPI y la base de datos MariaDB. El proyecto se divide en los siguientes pasos: Paso 1: Diseño de la Base de Datos Normalizada El primer paso consiste en diseñar la estructura de la base de datos, siguiendo las normas de normalización para garantizar la integridad y eficiencia de los datos almacenados. Paso 2: Generación de Datos En este paso, se procede a la generación de datos de prueba para poblar la base de datos. Para ello, se utilizó una combinación de datos reales y generados automáticamente con la ayuda de técnicas de Inteligencia Artificial para datos de prueba. Paso 3: Elaboración de KPI's de Control Financiero SQL Se elaboraron consultas SQL para calcular los Key Performance Indicators (KPI's) relacionados con el control financiero del negocio. Estos KPI's proporcionarán información clave para evaluar el rendimiento económico y financiero del negocio. Paso 4: Elaboración de la API En este paso, se creó una API utilizando FastAPI para proporcionar una interfaz de acceso a la base de datos. La API permite realizar operaciones CRUD (Crear, Leer, Actualizar y Eliminar) sobre los datos de la base de datos, lo que facilita la gestión y manipulación de la información almacenada. Paso 5: Mejora de la Presentación del Proyecto(pendiente) Este paso tiene como objetivo mejorar la presentación y documentación del proyecto para crear un diseño impactante. Se incluyen imágenes como el modelo de datos y capturas de pantalla de la API para una mejor comprensión visual. Modelo de Datos Captura de Pantalla de la API (Clientes) (https://postimg.cc/YhGQDFgq) Nota Este proyecto incluye tanto el diseño del modelo de datos como los registros de prueba generados de forma personalizada y mediante el uso de técnicas de IA para datos de prueba. Referencias Nota: Estan en constante mejora, siempre se puede mejorar un proyecto.",README.md,2023-12-20 21:52:40,https://github.com/programmingwithclaudio/api_negocio_tech,"{""proposito_principal"": ""API para gestionar una base de datos normalizada para negocios"", ""dominio_aplicacion"": ""Negocios/Finanzas"", ""tipo_proyecto"": [""API REST""], ""tecnologias_backend"": [""FastAPI""], ""tecnologias_frontend"": [], ""bases_datos"": [""MariaDB""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Operaciones CRUD"", ""KPI's de control financiero"", ""Base de datos normalizada""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""En desarrollo""]}"
programmingwithclaudio/app-auren-backend-flask,Sí,Sí,"🚀 Deploy Backend Flask en Railway 📁 Estructura de archivos necesarios para Railway 📝 1. Crear 📝 2. Crear (punto de entrada) 📝 3. Crear 📝 4. Actualizar para Railway 🔧 5. Actualizar para producción 🚀 6. Pasos para hacer deploy en Railway Paso 1: Preparar el repositorio Paso 2: Deploy en Railway Ve a railway.app Conecta tu cuenta de GitHub Click en ""Deploy from GitHub repo"" Selecciona tu repositorio del backend Railway detectará automáticamente que es una app Flask Paso 3: Configurar variables de entorno en Railway En el dashboard de Railway: Ve a la pestaña ""Variables"" Añade estas variables (NO subas el .env a git): 🔗 7. Conectar frontend local con backend en Railway Una vez deployado, Railway te dará una URL como: En tu frontend local, actualiza las URLs: 🛠️ 8. Comandos útiles ⚠️ 9. Consideraciones importantes CORS y Cookies El backend estará en El frontend en Necesitas para cross-domain Necesitas en HTTPS Variables de entorno NUNCA subas a git Añade a Configura todas las variables en Railway Dashboard Testing Prueba el health endpoint: Verifica que el login funcione desde tu frontend local Comprueba que las cookies se setean correctamente 🐛 10. Troubleshooting Si el frontend no puede conectarse: Si hay errores CORS: Si Railway no encuentra tu app: Verifica que esté en la raíz Verifica que esté en la raíz Verifica que esté correcto ✅ Checklist final creado creado creado Variables de entorno configuradas en Railway añadido a CORS configurado para tu IP local Frontend actualizado con nueva URL de Railway Cookies configuradas para cross-domain Health endpoint funcionando Login/auth funcionando desde frontend local ¡Tu backend estará corriendo en Railway y tu frontend local podrá conectarse perfectamente",README.md,2025-09-03 12:19:42,https://github.com/programmingwithclaudio/app-auren-backend-flask,"{""proposito_principal"": ""Backend Flask para deploy en Railway"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""API REST"", ""Backend Service""], ""tecnologias_backend"": [""Flask""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Railway"", ""GitHub""], ""funcionalidades_clave"": [""Health endpoint"", ""Login/auth"", ""CORS configuration"", ""Cookie management"", ""Cross-domain authentication""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Deployment"", ""Production"", ""Environment Variables"", ""HTTPS"", ""Testing""]}"
programmingwithclaudio/app-auren-developer,Sí,Sí,"Crear el tag anotado git tag -a 1415 -m ""update 13/08"" Enviar el tag al remoto git push origin 1415",README.md,2025-08-14 21:36:45,https://github.com/programmingwithclaudio/app-auren-developer,"{""proposito_principal"": ""Gestión de versiones y tags de Git para desarrollo de software"", ""dominio_aplicacion"": ""Desarrollo de Software"", ""tipo_proyecto"": [""Herramienta de desarrollo""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Git""], ""funcionalidades_clave"": [""Creación de tags anotados"", ""Sincronización de tags con repositorio remoto""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Git Workflow"", ""Version Control""]}"
programmingwithclaudio/app-auren-fronted-next,Sí,Sí,"npm install axios git tag -a v1.0.1 -m ""update a5c5d"" git push origin v1.0.1",README.md,2025-09-08 17:52:43,https://github.com/programmingwithclaudio/app-auren-fronted-next,"{""proposito_principal"": ""No se puede determinar el propósito específico del proyecto debido a la falta de documentación"", ""dominio_aplicacion"": ""No se puede determinar el dominio de aplicación"", ""tipo_proyecto"": [""Frontend""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": []}"
programmingwithclaudio/app-auren-supabase,Sí,Sí,"ETL de Ventas hacia Supabase Este repositorio contiene la automatización que extrae datos de SQL Server, los transforma y los envía a Supabase cada 30 minutos mediante un proceso de ETL. 📂 Archivo principal Script base de la ETL encargado de enviar las tablas procesadas hacia Supabase. 🏷️ Versionado con Git Tags Para generar una nueva versión se deben crear y enviar los tags correspondientes: SQLEXPRESSuserventassupabase.pyC:\Windows\System32\cmd.exe/c ""C:\eauren\taskforcedescarga\contronetdiario.bat""C:\eauren\taskforcedescargauserventas_supabase.py. ✍️ Autor: Equipo de Canal Fija 📅 Última actualización: 13/08",README.md,2025-08-18 14:43:08,https://github.com/programmingwithclaudio/app-auren-supabase,"{""proposito_principal"": ""ETL de Ventas hacia Supabase que extrae datos de SQL Server, los transforma y los envía a Supabase cada 30 minutos"", ""dominio_aplicacion"": ""Ventas"", ""tipo_proyecto"": [""ETL"", ""Pipeline de datos""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""SQL Server"", ""Supabase""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Extracción de datos de SQL Server"", ""Transformación de datos"", ""Carga de datos a Supabase"", ""Automatización cada 30 minutos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Automatización"", ""Data Pipeline"", ""Batch Processing""]}"
programmingwithclaudio/app-comfort-inn,No,Sí,"Comfort inn App - Versión0.0.6 (https://postimg.cc/jDz7F5Wn) ¡Dale like y suscríbete ProgrammingWithClaudio YouTube Channel para más contenido relacionado. Overview Este proyecto es una aplicación de gestión de reservas desarrollada con Flask en el backend y el fronted HTML, CSS Y Javascript, que proporciona una interfaz interactiva y analítica de datos. Imágenes de la Aplicación: Photo by PhotoMIX Company on PEXELS Photo by App Claudio Quispe on postimg.cc Características Interfaz intuitiva y fácil de usar. Se implementan reglas de negocio para las operaciones y diferentes funcionalidades. Despliegue sencillo con Docker por concluir. Backup con datos simulados. Tecnologías Utilizadas Docker 26.1.1 FLask =3.0.3 Python 3.10 Uso Clona el repositorio: Ejemplo de despliegue con Docker: ¡Explora la aplicación en Video presentación -Youtube Contribución Las contribuciones son bienvenidas. Si tienes ideas para nuevas características, por favor abre un issue para discutirlo o envía un pull request. Licencia Distribuido bajo la licencia MIT. Ver para más información.",README.md,2024-06-15 04:10:22,https://github.com/programmingwithclaudio/app-comfort-inn,"{""proposito_principal"": ""Aplicación de gestión de reservas con interfaz interactiva y analítica de datos"", ""dominio_aplicacion"": ""Hospitalidad/Hotelería"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Flask""], ""tecnologias_frontend"": [""HTML"", ""CSS"", ""JavaScript""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Gestión de reservas"", ""Interfaz interactiva"", ""Análisis de datos"", ""Reglas de negocio para operaciones""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""MIT License"", ""Docker Deployment""]}"
programmingwithclaudio/app-flask-users,Sí,Sí,"Chatbots Inicio 19/05 docker docker docker modificate maal base de datos Resoruces Deploy profesional git postgres-db CSS Cascade Style Sheets (hojas de estilo en cascada) Es un lenguaje que describe el renderizado de documentos estructurados como HTML o XML (SVG) Historia de CSS La web nace en el 91 y CSS en el 96. Hubo varios intentos de crear un lenguaje de estilos El primer navegador en implementar CSS fue IE3 Especificaciones de CSS Dic 96 - CSS1 https://www.w3.org/TR/REC-CSS1-961217 May 98 - CSS2 https://www.w3.org/TR/REC-CSS2/ 2011 - CSS 3 Aquí se divide CSS en módulos (antes era un monolito). No existe CSS4 https://developer.mozilla.org/es/docs/Web/CSS/CSS3 HTML5 https://www.w3.org/html/logo/ 20 años de CSS https://www.w3.org/Style/CSS20/ A brief history of CSS until 2016 CSS snapshot (estado actual de CSS) https://www.w3.org/TR/CSS/ Aquí se ponen las cosas ya terminadas, porque CSS se sigue trabajando. Búscadores implmentan css; css first css second ciudado con los usr agent es key sencitive en etiquetas pero no en atributos normazile css selectores de clase css specificity graph generator google fonts pexeles Los pilares Css Box Model Elemento de bloque Bordes y Clases Pseudoclases Fondos Color Texto Figma figma free Eliminación cascada y manual Para complementar la lógica de cascada de eliminación y permitir la eliminación de una reserva incluso si no tiene reservaciones asociadas, puedes ajustar la configuración de la relación entre y . Actualmente, estás utilizando una relación de uno a muchos entre estas dos entidades, pero no has configurado la cascada de eliminación. Aquí hay algunas opciones que puedes considerar: Agregar Cascada de Eliminación: Puedes configurar la relación entre y para que utilice la cascada de eliminación. Esto significa que cuando se elimina una reserva, todas las reservaciones asociadas también se eliminarán automáticamente. Esto se puede lograr agregando a la relación en la definición de la clase . Eliminar Manualmente las Reservaciones: Si prefieres no usar la cascada de eliminación y permitir la eliminación manual de una reserva incluso si no tiene reservaciones asociadas, puedes agregar una lógica adicional al eliminar la reserva en tu función en tus rutas. Con estas opciones, puedes elegir el enfoque que mejor se adapte a tus necesidades y preferencias en cuanto a la gestión de reservas y reservaciones en tu aplicación. Para implementar controles de tesorería, incluyendo el manejo de ingresos, egresos y flujo de...",README.md,2024-06-15 04:11:17,https://github.com/programmingwithclaudio/app-flask-users,"{""proposito_principal"": ""Aplicación Flask para gestión de usuarios con sistema de reservas y controles de tesorería"", ""dominio_aplicacion"": ""Gestión empresarial"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Flask""], ""tecnologias_frontend"": [""CSS"", ""HTML5""], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Git""], ""funcionalidades_clave"": [""Autenticación de usuarios"", ""Sistema de reservas"", ""Gestión de tesorería (ingresos/egresos)"", ""Eliminación en cascada de reservaciones""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Producción"", ""Deploy profesional""]}"
programmingwithclaudio/app-ia-multimodal,Sí,No,Sin documentación disponible,,2025-07-05 05:36:38,https://github.com/programmingwithclaudio/app-ia-multimodal,"{""proposito_principal"": ""No se puede determinar - sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar - sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/app-reports-avance,Sí,Sí,Proxy y Automatización App,README.md,2025-09-27 13:00:24,https://github.com/programmingwithclaudio/app-reports-avance,"{""proposito_principal"": ""Proxy y automatización para aplicaciones"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""Proxy"", ""Automatización""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/app-whatsapp-extractor,Sí,Sí,,README.md,2024-11-29 22:39:23,https://github.com/programmingwithclaudio/app-whatsapp-extractor,"{""proposito_principal"": ""Extracción de datos de WhatsApp"", ""dominio_aplicacion"": ""Herramientas de productividad"", ""tipo_proyecto"": [""CLI Tool"", ""Script""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Extracción de datos de WhatsApp"", ""Procesamiento de archivos de chat""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source""]}"
programmingwithclaudio/appjavademo,No,Sí,"Presentación del Proyecto - API con Spring Boot Objetivo del Proyecto El objetivo principal de este proyecto fue desarrollar una API utilizando Spring Boot para proporcionar servicios de creación, lectura, actualización y eliminación (CRUD) de entidades relacionadas con un sistema específico,en adelante, se evalua el framework ionic para el lado del cliente, y conectar todo el ecosistema de forma basica, dejando de lado aspectos especializados por el momento. Descripción General La API fue creada para interactuar con una base de datos MySQL y gestionar las entidades de clientes (Clientes). Se utilizó Spring Boot como marco de trabajo principal para facilitar el desarrollo y proporcionar una estructura modular y mantenible. Desafíos Configuración del Entorno Uno de los desafíos iniciales fue configurar correctamente el entorno de desarrollo, incluyendo la conexión a la base de datos MySQL y la integración con Spring Boot. Diseño de la Estructura de la API Definir la estructura de la API para implementar las operaciones CRUD con el modelo de entidad Cliente fue un desafío importante. Se debieron establecer rutas, métodos HTTP y servicios correspondientes para cada operación. Manejo de Excepciones y Validaciones La validación de datos de entrada y el manejo adecuado de excepciones para casos inesperados fueron aspectos cruciales para garantizar la integridad y seguridad de la API. Pruebas y Depuración Realizar pruebas exhaustivas y depurar para garantizar el correcto funcionamiento de cada endpoint y la integración adecuada con la base de datos fue otro desafío esencial. Logros Implementación Exitosa de la API Se logró implementar una API funcional y robusta que permitía realizar operaciones CRUD en la entidad Cliente. Conexión a la Base de Datos La correcta conexión y gestión de la base de datos MySQL mediante Spring Boot se logró con éxito, permitiendo operaciones fluidas en los datos. Estructura Modular y Mantenible Se diseñó una estructura modular y mantenible siguiendo las mejores prácticas de desarrollo de Spring Boot, lo que facilita la escalabilidad y futuras expansiones del proyecto. Manejo de Excepciones y Validaciones Se implementaron adecuadamente validaciones de entrada y manejo de excepciones, asegurando un comportamiento predecible y seguro de la API. Conclusiones La implementación de la API con Spring Boot proporcionó una solución efectiva para la gestión de entidades Cliente. Los desafíos enfrentados fortalecieron la comprensión del marco de...",README.md,2024-01-14 20:43:22,https://github.com/programmingwithclaudio/appjavademo,"{""proposito_principal"": ""API para gestión de entidades Cliente con operaciones CRUD"", ""dominio_aplicacion"": ""Sistema de gestión de clientes"", ""tipo_proyecto"": [""API REST"", ""Full Stack Web""], ""tecnologias_backend"": [""Spring Boot""], ""tecnologias_frontend"": [""Ionic""], ""bases_datos"": [""MySQL""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Operaciones CRUD"", ""Manejo de excepciones"", ""Validaciones de datos"", ""Conexión a base de datos MySQL""], ""lenguajes_programacion"": [""Java""], ""tags_adicionales"": [""Demo"", ""Proyecto educativo""]}"
programmingwithclaudio/AppSupermarket2_nginx,Sí,Sí,"This is a Next.js project bootstrapped with (https://github.com/vercel/next.js/tree/canary/packages/create-next-app). Getting Started First, run the development server: Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying . The page auto-updates as you edit the file. This project uses (https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font. Learn More To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.",README.md,2024-03-23 07:38:44,https://github.com/programmingwithclaudio/AppSupermarket2_nginx,"{""proposito_principal"": ""Aplicación web desarrollada con Next.js para fines no especificados en la documentación proporcionada"", ""dominio_aplicacion"": ""Desarrollo web"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Optimización de fuentes"", ""Desarrollo local con hot reload"", ""Despliegue en Vercel""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Template"", ""Open Source""]}"
programmingwithclaudio/AppSupermarket_nginx,Sí,No,Sin documentación disponible,,2024-03-22 22:07:32,https://github.com/programmingwithclaudio/AppSupermarket_nginx,"{""proposito_principal"": ""No se puede determinar el propósito específico del proyecto debido a la falta de documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar el dominio de aplicación específico"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/app_client_demo,No,No,Sin documentación disponible,,2023-12-29 16:19:37,https://github.com/programmingwithclaudio/app_client_demo,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/app_datadiip_dashboard,Sí,No,Sin documentación disponible,,2024-04-06 22:34:15,https://github.com/programmingwithclaudio/app_datadiip_dashboard,"{""proposito_principal"": ""Dashboard para visualización y análisis de datos"", ""dominio_aplicacion"": ""Data Science"", ""tipo_proyecto"": [""Dashboard"", ""Full Stack Web""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/app_finance_reports,No,Sí,"Nombre del Proyecto Descripción breve o tagline del proyecto. Tabla de Contenidos Descripción Características Capturas de Pantalla Instalación Uso Contribución Créditos Licencia Descripción Una breve descripción del proyecto, su propósito y sus principales características. Características Lista de las principales características y funcionalidades del proyecto. Capturas de Pantalla Esta API proporciona datos de facturación y ventas para una tienda en línea. Permite realizar solicitudes HTTP GET para obtener información detallada sobre las facturas y los productos vendidos. Los datos se devuelven en formato JSON. Endpoints Descripción Este endpoint devuelve todos los elementos o productos vendidos almacenados en la base de datos. (https://postimg.cc/34d1m9J4) Descripción La aplicación de Insights (https://postimg.cc/94fYH198) Descripción Visualización de las ventas más recientes con detalles como número de factura, estado y monto total. (https://postimg.cc/dZG8jfM1) Descripción Visualización de las ventas más recientes con detalles como número de factura, estado y monto total. (https://postimg.cc/9rjPQcTq) Instalación Instrucciones sobre cómo instalar y configurar el proyecto. Incluye cualquier requisito previo, dependencias, comandos de instalación, etc. Uso Explica cómo usar el proyecto, proporciona ejemplos de código si es necesario y describe cualquier configuración o personalización que el usuario pueda necesitar. Contribución Indica cómo otras personas pueden contribuir al proyecto. Puedes incluir pautas de contribución, información sobre cómo informar problemas o sugerencias, y cómo enviar solicitudes de incorporación de cambios (pull requests). Créditos Agradece y da créditos a cualquier persona o recurso que haya sido de ayuda para el proyecto. Licencia Indica la licencia bajo la cual se distribuye el proyecto. En este caso es libre de uso para todo el que desee aprender esta es mi contribución a los Tutoriales de YouTube, IA's y Stackoverflow . ¡Gracias por revisar este README Si tienes alguna pregunta o sugerencia, no dudes en contactarnos.",README.md,2024-03-14 00:55:04,https://github.com/programmingwithclaudio/app_finance_reports,"{""proposito_principal"": ""API para gestión de facturación y ventas de una tienda en línea"", ""dominio_aplicacion"": ""E-commerce"", ""tipo_proyecto"": [""API REST"", ""Dashboard""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Obtención de datos de facturación"", ""Obtención de datos de productos vendidos"", ""Visualización de ventas recientes"", ""Dashboard de insights""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""Educational""]}"
programmingwithclaudio/asistencia-transactions-taskforce-pasado,Sí,Sí,"AUTOMATIZACIÓN TASKFORCE-CONTROLNET Requisitos según detalles Google chrome-version-Version: 135.0.7049.95 chrome-driver Intalar librerías Credenciales de google Procedimiento ACTUALIZACIÓN INICIAL DIARIA Descarga Manual de archivos , , Ruta de directorio: Abrir directorio con VSCODE Ejecutar ACTUALIZACIÓN DIA ANTERIOR Descarga Manual de archivos , Ruta de directorio: Abrir directorio con VSCODE Ejecutar CAPTURAS: Ruta de Archivos Descargados - Abrir y Ejecutar https://doc.evolution-api.com/v1/en/install/nvm https://docs.n8n.io/hosting/installation/npm/",README.md,2025-05-03 19:06:58,https://github.com/programmingwithclaudio/asistencia-transactions-taskforce-pasado,"{""proposito_principal"": ""Automatización de procesos de descarga y procesamiento de archivos para taskforce de controlnet"", ""dominio_aplicacion"": ""Automatización de procesos"", ""tipo_proyecto"": [""Script de automatización"", ""CLI Tool""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Descarga manual de archivos"", ""Ejecución de actualizaciones diarias"", ""Procesamiento de archivos descargados""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Automatización"", ""Taskforce"", ""ControlNet"", ""Procesamiento de archivos""]}"
programmingwithclaudio/backend_next,Sí,Sí,"MongoDB and Mongoose with Next.js This example shows how you can use a MongoDB database to support your Next.js application. Pet is an application that allows users to add their pets' information (e.g., name, owner's name, diet, age, dislikes, likes, and photo). They can also delete it or edit it anytime. Deploy your own Once you have access to the environment variables you'll need, deploy the example using Vercel: (https://vercel.com/new/clone?repository-url=https://github.com/vercel/next.js/tree/canary/examples/with-mongodb-mongoose&project-name=with-mongodb-mongoose&repository-name=with-mongodb-mongoose&env=MONGODBURI&envDescription=Required%20to%20connect%20the%20app%20with%20MongoDB&envLink=https://github.com/vercel/next.js/tree/canary/examples/with-mongodb-mongoose%23step-2-set-up-environment-variables) How to use Execute (https://github.com/vercel/next.js/tree/canary/packages/create-next-app) with npm, Yarn, or pnpm to bootstrap the example: Configuration Step 1. Get the connection string of your MongoDB server In the case of MongoDB Atlas, it should be a string like this: For more details, follow this MongoDB Guide on how to connect to MongoDB. Step 2. Set up environment variables Copy the file in this directory to (which will be ignored by Git): Then set each variable on : should be the MongoDB connection string you got from step 1. Step 3. Run Next.js in development mode Your app should be up and running on http://localhost:3000 If it doesn't work, post on GitHub discussions. Deploy on Vercel You can deploy this app to the cloud with Vercel (Documentation). Deploy Your Local Project To deploy your local project to Vercel, push it to GitHub/GitLab/Bitbucket and import to Vercel. Important: When you import your project on Vercel, make sure to click on Environment Variables and set them to match your file. Deploy from Our Template Alternatively, you can deploy using our template by clicking on the Deploy button below. (https://vercel.com/new/clone?repository-url=https://github.com/vercel/next.js/tree/canary/examples/with-mongodb-mongoose&project-name=with-mongodb-mongoose&repository-name=with-mongodb-mongoose&env=MONGODBURI&envDescription=Required%20to%20connect%20the%20app%20with%20MongoDB&envLink=https://github.com/vercel/next.js/tree/canary/examples/with-mongodb-mongoose%23step-2-set-up-environment-variables)",README.md,2024-04-15 03:58:07,https://github.com/programmingwithclaudio/backend_next,"{""proposito_principal"": ""Aplicación para gestionar información de mascotas (nombre, dueño, dieta, edad, gustos, disgustos y foto)"", ""dominio_aplicacion"": ""Gestión de datos personales"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [""MongoDB"", ""Mongoose""], ""ml_ia"": [], ""devops_cloud"": [""Vercel""], ""funcionalidades_clave"": [""CRUD de mascotas"", ""Gestión de información de mascotas"", ""Subida de fotos""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Template"", ""Example Project"", ""Open Source""]}"
programmingwithclaudio/books-algoritmos-project,Sí,Sí,"title: ""Aplicaciones de Límites y Continuidad en Diversas Disciplinas"" author: ""Tu Nombre"" date: ""Fecha"" output: htmldocument: toc: true tocdepth: 2 Introducción En este artículo, exploraremos diversas aplicaciones de los conceptos de límites y continuidad en matemáticas y otras áreas. A lo largo del texto, se presentarán ejemplos con explicaciones detalladas. Además, se incluirán gráficos para visualizar los casos de las ecuaciones. https://scikit-learn.org/stable/datasets.html Límites y Continuidad 2.1 Cálculo de Límites Ejemplo 1: Calcular el límite de la función $f(x) = \frac$ cuando $x$ se acerca a $1$. $$ \lim \frac = \lim (x + 1) = 2 $$ Este límite nos indica que, a medida que $x$ se acerca a $1$, los valores de $f(x)$ se acercan a $2$. 2.2 Teoría de Números Ejemplo 2: Calcular el límite de la sucesión $an = \frac$ cuando $n$ tiende a infinito. $$ \lim \frac = \frac $$ Este límite indica que a medida que $n$ se vuelve extremadamente grande, los términos de la sucesión $an$ se acercan a $\frac$. 2.3 Optimización Ejemplo 3: En un problema de optimización, minimizar el perímetro de un rectángulo con área fija. Calculamos el límite de $2x + 2y$ cuando $x$ se acerca a $y$. $$ \lim (2x + 2y) $$ Este límite nos permite encontrar el valor mínimo del perímetro. Para resolver el problema de optimización dado y calcular el límite de \(2x + 2y\) cuando \(x\) se acerca a \(y\), puedes seguir estos pasos: Definir el Problema de Optimización: En este caso, el problema es minimizar el perímetro de un rectángulo con un área fija. Para ello, puedes definir las siguientes variables: \(x\) y \(y\) representan las dimensiones del rectángulo. El área del rectángulo es \(A = xy\). El perímetro del rectángulo es \(P = 2x + 2y\). También tienes una restricción en el área: \(A = xy = \text\). Expresar una Variable en Términos de la Otra: Puedes despejar una de las variables en términos de la otra usando la restricción de área. Por ejemplo, puedes expresar \(y\) en términos de \(x\): $(y = \frac)$ Expresar la Función de Perímetro en Términos de una Variable: Sustituye \(y\) en la función de perímetro \(P = 2x + 2y\) con la expresión que obtuviste en el paso anterior. $$ P(x) = 2x + 2\left(\frac\right) $$ Calcular el Límite: Ahora, calcula el límite de \(P(x)\) cuando \(x\) se acerca a \(y\): $$ \lim P(x) = \lim \left(2x + 2\left(\frac\right)\right) $$ Resuelve el Límite: Utiliza las reglas de límites para calcular el valor mínimo del perímetro. $$ \lim P(x) = 2y +...",README.md,2024-07-23 23:39:56,https://github.com/programmingwithclaudio/books-algoritmos-project,"{""proposito_principal"": ""Documentación educativa sobre aplicaciones de límites y continuidad en matemáticas"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Documentación"", ""Artículo académico""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Ejemplos matemáticos detallados"", ""Gráficos para visualización de ecuaciones"", ""Cálculo de límites"", ""Aplicaciones en teoría de números"", ""Problemas de optimización""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Educativo"", ""Matemáticas"", ""Cálculo"", ""Límites"", ""Continuidad""]}"
programmingwithclaudio/bot-telegram-py,Sí,Sí,El código ahora incluye: Métodos completos de la clase WhatsAppImageMonitor: verificarconexionopenwa() - Verifica conexión con OpenWA verificargrupoobjetivo() - Verifica acceso al grupo obtenertodoslosmensajesgrupo() - Obtiene mensajes históricos descargarydesencriptarimagen() - Método principal de desencriptación iniciarmonitor() - Método principal que ejecuta todo Todos los métodos auxiliares de filtrado y normalización Funcionalidad de desencriptación completa: Algoritmo AES-CBC con HKDF para derivar claves Detección automática de formatos de imagen Verificación de integridad de datos desencriptados Manejo de errores específicos para desencriptación El código ahora debería ejecutarse sin errores de métodos faltantes. Asegúrate de tener instalado: bashpip install cryptography Y luego ejecutar: bashpython whastapploadhist.py ocr de iamgenes https://github.com/UB-Mannheim/tesseract/wiki instalar C:\eauren\datalakeavwhastapp\rechazos\.jpg C:\Users\oak\AppData\Local\Programs\Tesseract-OCR\tesseract.exe C:\Users\oak\AppData\Local\Programs\Tesseract-OCR\tessdata\spa.traineddata,README.md,2025-09-16 19:49:48,https://github.com/programmingwithclaudio/bot-telegram-py,"{""proposito_principal"": ""Bot de Telegram para monitoreo y procesamiento de imágenes de WhatsApp mediante desencriptación y OCR"", ""dominio_aplicacion"": ""Automatización de mensajería"", ""tipo_proyecto"": [""Bot"", ""CLI Tool"", ""Script de procesamiento""], ""tecnologias_backend"": [""Python""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""OCR"", ""Tesseract""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Desencriptación AES-CBC con HKDF"", ""Monitoreo de grupos de WhatsApp"", ""Procesamiento de imágenes"", ""Extracción de texto con OCR"", ""Verificación de integridad de datos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Experimental""]}"
programmingwithclaudio/bots_whatsapp_send_reports,Sí,Sí,"Resets Descarta todos los cambios locales (incluso archivos nuevos no rastreados) git reset --hard Elimina archivos no rastreados (logs, temporales, etc.) git clean -fd Trae la última versión del repositorio remoto git fetch origin Fuerza tu rama local a ser igual a la del remoto git reset --hard origin/main",README.md,2025-10-28 13:35:34,https://github.com/programmingwithclaudio/bots_whatsapp_send_reports,"{""proposito_principal"": ""Script para enviar reportes automatizados por WhatsApp"", ""dominio_aplicacion"": ""Automatización y Mensajería"", ""tipo_proyecto"": [""CLI Tool"", ""Bot""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Envío automatizado de mensajes"", ""Integración con WhatsApp""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source""]}"
programmingwithclaudio/cashtrackr,Sí,Sí,"dev components tailwind react icons docker .env and docker-compose.yml createdatabase prisma developer fronted useFormState useActionState setion 50 video 7 server componentes para traer datos desde orm, fecth api e informacion, autenticacion, api keys o token client componentes onclick, onsubmit, onchange, etc. Hooks useState, useEffect y useReducer.LIbrerias Toast, Zustand y otras del cliente. LocalStorage notification api, geolocation api, api externa copn react query o otras liberias la primera vez se ejecuta e el servidor despues en el cliente , se llama hidratacion server actions son funciones asincronas que se ejecutan en el servidor y pueden utilizar componentes de cliente y servidor nexts js cookiers son un dynamic function que su valor de retorno no se puede saber por anticipado headlessui PasswordResetHandler.tsx ResetPasswordForm.tsx ValidateTokenForm.tsx validate-token-action.ts",README.md,2025-01-30 01:08:56,https://github.com/programmingwithclaudio/cashtrackr,"{""proposito_principal"": ""Sistema de seguimiento y gestión de finanzas personales"", ""dominio_aplicacion"": ""Finanzas"", ""tipo_proyecto"": [""Full Stack Web"", ""Dashboard""], ""tecnologias_backend"": [""Next.js"", ""Prisma""], ""tecnologias_frontend"": [""React"", ""Tailwind CSS"", ""Headless UI""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Autenticación"", ""API keys/tokens"", ""LocalStorage"", ""Notifications API"", ""Geolocation API"", ""Server Actions"", ""Cookies"", ""Password reset"", ""Token validation""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Open Source"", ""Production"", ""ORM"", ""React Query"", ""Zustand"", ""Toast notifications""]}"
programmingwithclaudio/cluster_local_lendingclub,No,Sí,"Proyecto de Clústeres local con Hadoop, Pyspark y Airflow This dataset contains the full LendingClub data available from their site. There are separate files for accepted and rejected loans. The accepted loans also include the FICO scores, which can only be downloaded when you are signed in to LendingClub and download the data (https://www.kaggle.com/datasets/wordsforthewise/lending-club). Inicio de Docker y Contenedores Cluster local Descarga los archivos y ubicalos de kaggle 1Gb, según directorio muestra Migración de archivos comprimidos al cluster Desarrollo del Proyecto Valida la clave de inicio de notebook en los logs del contedor Previo a la carga airflow valida la creación y permisos de carpetas que requiere airflow Referencias: Docker. (2024). Docker.com. https://hub.docker.com/u/bde2020 George, N. (2018). All Lending Club loan data (Version 3) Data set. Kaggle. https://www.kaggle.com/wordsforthewise/lending-club",README.md,2024-01-27 23:23:40,https://github.com/programmingwithclaudio/cluster_local_lendingclub,"{""proposito_principal"": ""Análisis de datos de préstamos de LendingClub usando clúster local con Hadoop, PySpark y Airflow"", ""dominio_aplicacion"": ""Finanzas"", ""tipo_proyecto"": [""Data Pipeline"", ""Data Analysis""], ""tecnologias_backend"": [""PySpark""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Hadoop"", ""Airflow""], ""funcionalidades_clave"": [""Procesamiento de datos distribuido"", ""Orquestación de workflows"", ""Análisis de datos financieros""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Data Processing"", ""Cluster Computing""]}"
programmingwithclaudio/coding_c_plus,No,Sí,,README.md,2025-10-22 04:25:33,https://github.com/programmingwithclaudio/coding_c_plus,"{""proposito_principal"": ""No se puede determinar - falta información del proyecto"", ""dominio_aplicacion"": ""No se puede determinar - falta información del proyecto"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/consolidado-etl-sql,Sí,Sí,Querys Filters Tabla final- reintento rezagada,README.md,2024-08-22 20:44:29,https://github.com/programmingwithclaudio/consolidado-etl-sql,"{""proposito_principal"": ""Procesamiento ETL con filtros y reintentos para datos rezagados"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""ETL Pipeline"", ""Data Processing""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Filtros de consultas"", ""Tabla final consolidada"", ""Reintentos para datos rezagados""], ""lenguajes_programacion"": [""SQL""], ""tags_adicionales"": []}"
programmingwithclaudio/controlnet-automatizacion-holders,Sí,Sí,"AUTOMATIZACIÓN TASKFORCE-CONTROLNET Requisitos según detalles Google chrome-version-Version: 135.0.7049.95 chrome-driver Intalar librerías Credenciales de google Procedimiento ACTUALIZACIÓN INICIAL DIARIA Descarga Manual de archivos , , Ruta de directorio: Abrir directorio con VSCODE Ejecutar ACTUALIZACIÓN DIA ANTERIOR Descarga Manual de archivos , Ruta de directorio: Abrir directorio con VSCODE Ejecutar CAPTURAS: Ruta de Archivos Descargados - Abrir y Ejecutar https://doc.evolution-api.com/v1/en/install/nvm https://docs.n8n.io/hosting/installation/npm/",README.md,2025-10-16 22:02:40,https://github.com/programmingwithclaudio/controlnet-automatizacion-holders,"{""proposito_principal"": ""Automatización de tareas de captura y procesamiento de datos para taskforce ControlNet"", ""dominio_aplicacion"": ""Automatización de procesos"", ""tipo_proyecto"": [""Script de automatización"", ""Task automation""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Descarga manual de archivos"", ""Ejecución de scripts de actualización diaria"", ""Gestión de rutas de directorio"", ""Captura de datos""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Google Chrome"", ""Chrome Driver"", ""Evolution API"", ""n8n"", ""nvm""]}"
programmingwithclaudio/control_cortes_automaticos,Sí,No,Sin documentación disponible,,2025-10-28 14:46:33,https://github.com/programmingwithclaudio/control_cortes_automaticos,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/CP05_20FT_01,Sí,Sí,"Carrera Data Science - Módulo 1 Bienvenido al Checkpoint del Módulo 1 de la Carrera Data Science. ¡ Por favor lee TODO este material con atención INTRODUCCION El Checkpoint es un desafío técnico donde evaluamos conceptos vistos en el módulo. En esta instancia, buscamos asegurarnos que todos nuestros aplicantes tengan una base de conocimientos mínimos necesarios para luego seguir aprendiendo temas nuevos. Debe resolverse de manera individual. Si te copias o recibes ayuda de compañeros, además de estar incumpliendo con las normas de Henry (lo que te dejaría afuera de la carrera), estarás perjudicándote a ti mismo, porque el primer día de clase estarías extremadamente perdido. PASOS PARA RESOLVER EL CHECKPOINT: FORK Primero debes forkear este repo, haciendo click en el botón de arriba a la derecha. Una vez que tengas una copia de este repo en tu cuenta de , cloná el repo dentro de una nueva carpeta (en nuestro ejemplo, va a ser ""checkpoint_m1""). Asegurate de no utilizar la misma que el prep curse. Una vez clonado entrá a esa carpeta y ejecutá los siguientes comandos: python tests.py Si ves los tests fallando, estás listo para comenzar, si no lee bien el output para identificar cual es el error. RESOLVER EL CHECKPOINT Tu tarea es completar el código en de tal forma que pasen la mayoría de los tests. ENTREGAR TU CHECKPOINT Correr por ultima vez los tests y verificar cuantos pasan. Ten en cuenta que si te aparece ""1 failed;1 total"" es porque tienes un error de sintaxis: seguramente falta o sobra un corchete, paréntesis, dos puntos, etc. Saca un print de pantalla de tus tests. Luego, debes subir un commit a tu repo. Para hacerlo, debes ejecutar el siguiente comando: git add . git commit -m 'checkpoint commit' git push origin main Una vez finalizado, chequea: Que veas los cambios reflejados en el repo de tu cuenta de github (entrando a tu repo desde el browser.) Que no haya un require - solo debe haber codigo dentro de las funciones de cada ejercicio Atención: no debes realizar un commit después de la hora de entrega porque se anulara la totalidad del examen. Revisar la hora del entrega del examen en los emails que te llegaron. ¿TENES ALGUN PROBLEMA / CONSULTA? Si te ocurre algún problema, revisa los canales en Slack. Probablemente a algún compañero le paso algo similar y ya lo consulto. Si no encuentras la respuesta, puedes publicar un mensaje en dicho canal. No se puede hacer consultas sobre la resolucion de los ejercicios. GUIA DE ERRORES COMUNES Para...",README.md,2023-12-01 16:18:29,https://github.com/programmingwithclaudio/CP05_20FT_01,"{""proposito_principal"": ""Evaluación técnica de conocimientos básicos de Data Science para estudiantes de Henry"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Evaluación técnica"", ""Checkpoint académico""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Ejecución de tests automatizados"", ""Evaluación de código Python"", ""Verificación de sintaxis""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Educativo"", ""Evaluación"", ""Checkpoint"", ""Henry"", ""Data Science""]}"
programmingwithclaudio/creditrisk-app,Sí,Sí,"Desarrollo de apps MODELS: Table, Column, Unique, AllowNull, Model, DataType, ForeignKey, BelongsTo, HasMany, Index / TYPES: LoanStatus, EmploymentStatus CONTROLLER : getAll- create- getById- updateById- deleteById VALIDACIONES : (authenticate) - validateBudgetId - validateBudgetExists - hasAccess / validateBudgetInput - handleInputErrors ROUTE : (use) - param - get - post - put -delete Crear el archivo : Modificar el archivo : Reinstalar dependencias con : Esto volverá a instalar todas las dependencias con las nuevas versiones correctas y evitará el error relacionado con . Comando para instalar las dependencias correctamente: Si prefieres no modificar manualmente el archivo , puedes instalar las dependencias nuevamente pero excluyendo la versión problemática de o fijando la versión correcta directamente durante la instalación. Esto lo puedes hacer con el siguiente comando: Desarrollar base de datos Implementar Logica de muchos a muchos En una base de datos, las relaciones de muchos a muchos (many-to-many) se manejan mediante una tabla intermedia o de unión (también conocida como tabla de unión o tabla puente). Esta tabla intermedia contiene las claves foráneas de las dos tablas que se relacionan, permitiendo que múltiples registros de una tabla se relacionen con múltiples registros de otra tabla. En tu caso, si identificas que falta una relación de muchos a muchos, puedes agregar una tabla intermedia para manejar esa relación. Por ejemplo, si quisieras que un (Prestatario) pueda tener múltiples y un pueda aplicarse a múltiples , necesitarías una tabla intermedia para manejar esta relación. Aquí te muestro cómo podrías estructurar una relación de muchos a muchos entre y : Explicación: Borrower: Contiene la información del prestatario. CreditScoreModel: Contiene la información del modelo de puntaje crediticio. BorrowerCreditScoreModel: Es la tabla intermedia que maneja la relación de muchos a muchos entre y . Esta tabla tiene dos claves foráneas: : Hace referencia al de la tabla . : Hace referencia al de la tabla . Ejemplo de uso: Un puede tener múltiples . Un puede aplicarse a múltiples . Ejemplo de datos: Borrower: - CreditScoreModel: - BorrowerCreditScoreModel: (Juan Perez tiene el modelo A) (Juan Perez también tiene el modelo B) (Maria Lopez tiene el modelo A) Modelo de datos cloud --- Diferencias Clave: | Característica | Código Postal | Ubigeo | |---------------|-------------|--------| | Institución que lo asigna | Serpost | INEI | | Número de...",README.md,2025-02-05 06:35:37,https://github.com/programmingwithclaudio/creditrisk-app,"{""proposito_principal"": ""Sistema de gestión de riesgo crediticio para préstamos"", ""dominio_aplicacion"": ""Finanzas"", ""tipo_proyecto"": [""API REST"", ""Full Stack Web""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Autenticación"", ""Validación de presupuestos"", ""Gestión de préstamos"", ""Relaciones muchos a muchos entre prestatarios y modelos de puntaje crediticio"", ""CRUD completo para entidades""], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/crud-appscript-lookerstudio,Sí,Sí,"crud-appscript-lookerstudio crud-appscript-lookerstudio es una aplicación que tiene como base de datos google sheets y utiliza una interfaz gráfica para gestion de ingreso, actualización y eliminación registros y finalmente conecta con looker studio para parte gráfica Crear Modelo de Datos en e Iniciar App Scripts , copiar los script desarrollados en el folder Habilitar la opcion de servcios Ingresar a Looker compartir y obtener el enlace flowchart App Scripts Looker Studio Materiales script lookerstudio",README.md,2025-02-24 06:20:37,https://github.com/programmingwithclaudio/crud-appscript-lookerstudio,"{""proposito_principal"": ""Aplicación CRUD con base de datos Google Sheets e interfaz gráfica para gestión de registros, conectada con Looker Studio para visualización de datos"", ""dominio_aplicacion"": ""Business Intelligence / Data Visualization"", ""tipo_proyecto"": [""Full Stack Web"", ""Dashboard"", ""Data Integration""], ""tecnologias_backend"": [""Google Apps Script""], ""tecnologias_frontend"": [], ""bases_datos"": [""Google Sheets""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Operaciones CRUD (Create, Read, Update, Delete)"", ""Integración con Google Sheets como base de datos"", ""Conexión con Looker Studio para visualización"", ""Interfaz gráfica para gestión de registros""], ""lenguajes_programacion"": [""JavaScript""], ""tags_adicionales"": [""Google Workspace Integration"", ""Data Management"", ""Business Intelligence""]}"
programmingwithclaudio/dev_ia_dbaw,Sí,Sí,Sentiment Analysis Named Entity Recognition Question Answering con Context Text Summarization Translation Classification Text Generation Image Generation Audio Generation VERSION OPTIMIZADA PARA 8GB RAM Usa esta versión si el código anterior consume mucha memoria ✅ Cómo Harvey está revolucionando los servicios legales mediante LLMs. ✅ Cómo Nebula.io está transformando la gestión del talento y reclutamiento con IA. ✅ Cómo Bloop.ai está resolviendo los desafíos del código legado utilizando LLMs. ✅ Implementación de Salesforce Einstein Copilot Health Actions en el sector salud. ✅ Cómo Khan Academy está mejorando la educación mediante LLMs.,README.md,2025-11-10 01:32:01,https://github.com/programmingwithclaudio/dev_ia_dbaw,"{""proposito_principal"": ""Demostración y análisis de casos de uso de LLMs en diferentes industrias"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Análisis de Casos de Estudio"", ""Documentación Técnica""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""LLMs""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Análisis de casos de uso de IA"", ""Documentación de implementaciones de LLMs""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Casos de Estudio"", ""Documentación"", ""Análisis de IA""]}"
programmingwithclaudio/docker_airflow_cluster_v1,No,Sí,"Reporte para el Análisis de estudios de mercado en segmentos. (https://github.com/programmingwithclaudio/iachatlangchain) (https://github.com/programmingwithclaudio/iachatlangchain) Este proyecto implementa una arquitectura de datos completa que orquesta el proceso de scraping, staging y carga a MinIO como utilizando Airflow. Objetivo: Automatizar un flujo de análisis de estudios de mercado para las importaciones en Perú. Se actualizará cada 15 días. Flujo de operaciones en la Orquestación con Airflow Scraping y Extracción: Se utiliza Selenium y BeautifulSoup para extraer dinámicamente los enlaces de archivos ZIP de la web de Aduanas. Staging y Persistencia: Se implementa temporalmente un directorio de staging y se registra su metadata (nombre, URL, fecha, estado y rutas de staging/MinIO) en una base de datos PostgreSQL mediante psycopg2. Carga a MinIO: Utilizando boto3, los archivos validados se suben a un bucket en MinIO para un almacenamiento centralizado , Orquestación con Airflow: Todo el flujo está orquestado mediante un DAG de Airflow que coordina las tareas de scraping, descarga y carga, garantizando robustez, escalabilidad y manejo de fallos a través de políticas de reintentos y control de estados. Despliegue y Escalabilidad: La infraestructura se levanta con Docker Compose, integrando servicios clave (Postgres, Redis, MinIO, Kafka, Spark) para soportar un ecosistema de datos distribuido y listo para pipelines ETL avanzados. Esta solución permite la integración de datos desde fuentes web, su validación y almacenamiento centralizado, sirviendo como base para procesos de análisis y transformación de datos en entornos empresariales. Requisitos: Git Docker, Docker-Compose Linux o Windows con (WSL) Nota: ArchiLinux +XFCE (Desktop) la mejor distribución Comandos del proyecto: Referencias ""Designing Data-Intensive Applications"" (Martin Kleppmann, 2017) ""Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing"" (Tyler Akidau et al., 2018)",README.md,2025-06-24 13:07:40,https://github.com/programmingwithclaudio/docker_airflow_cluster_v1,"{""proposito_principal"": ""Automatizar un flujo de análisis de estudios de mercado para las importaciones en Perú mediante scraping, staging y carga de datos"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""Pipeline de datos"", ""ETL"", ""Orquestación""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose""], ""funcionalidades_clave"": [""Scraping web con Selenium y BeautifulSoup"", ""Orquestación con Airflow DAG"", ""Carga a MinIO"", ""Manejo de estados y reintentos"", ""Validación de archivos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Producción"", ""Arquitectura de datos distribuida""]}"
programmingwithclaudio/dotfiles,No,Sí,"Available with Ubuntu/Debian | Archlinux Dotfiles (https://github.com/morhetz/gruvbox) (https://neovim.io) (https://github.com/programmingwithclaudio/dotfiles) (https://opensource.org/licenses/MIT) Requirements: Git Node yay Clone dotfiles or download Ejecuta el siguiente comando en tu terminal para clonar el repositorio que contiene tus configuraciones personalizadas: EXEC script settings: Una vez clonado el repositorio, ejecuta el script que configura tu entorno: Reemplaza los files basicos de configuracion por los de la repo Reemplaza los files basicos de configuracion por los de la repo",README.md,2025-03-11 22:45:56,https://github.com/programmingwithclaudio/dotfiles,"{""proposito_principal"": ""Configuración y personalización de entorno de desarrollo mediante dotfiles"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""CLI Tool"", ""Configuration Management""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Git""], ""funcionalidades_clave"": [""Reemplazo de archivos de configuración"", ""Script de configuración automática"", ""Personalización de entorno de desarrollo""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""MIT License"", ""Dotfiles"", ""Neovim"", ""Gruvbox Theme""]}"
programmingwithclaudio/enero-uptask-mern,Sí,No,Sin documentación disponible,,2025-01-04 00:40:01,https://github.com/programmingwithclaudio/enero-uptask-mern,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/etl-process-operator,Sí,Sí,"Voy a ayudarte a integrar Prefect y organizar las dependencias necesarias. Para instalar las dependencias, necesitas ejecutar: Las principales mejoras con Prefect incluyen: Orquestación de tareas: Cada operación importante es ahora una tarea de Prefect (@task) El flujo principal está decorado con @flow Reintentos automáticos para operaciones propensas a fallar Caché y optimización: Caché de conexiones a base de datos Reintentos configurados para operaciones de DB Control de tiempo de expiración de caché Monitoreo y logging: Mejor trazabilidad de errores Monitoreo de estado de tareas Métricas de ejecución Para ejecutar el flujo con la UI de Prefect, necesitas: Iniciar el servidor Prefect: En otra terminal, iniciar un worker: Ejecutar el script: También puedes programar el flujo para que se ejecute periódicamente:",README.md,2025-03-11 04:09:27,https://github.com/programmingwithclaudio/etl-process-operator,"{""proposito_principal"": ""Orquestación y automatización de procesos ETL (Extract, Transform, Load) usando Prefect"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""ETL Pipeline"", ""Data Pipeline""], ""tecnologias_backend"": [""Prefect""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Orquestación de tareas"", ""Reintentos automáticos"", ""Caché de conexiones a base de datos"", ""Monitoreo de estado de tareas"", ""Métricas de ejecución"", ""Programación periódica de flujos""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source""]}"
programmingwithclaudio/etl_postgres_gaming,No,Sí,Proyecto ETL postgres para Rubro Gaming y visualización grafana Justificación de Proyecto Youtube Presentación Integrador Gaming Complementos de con otras tecnologias Esquema relacional (https://postimg.cc/R6dx6DVd) Grafana (https://postimg.cc/Z04QBHNB),README.md,2024-02-01 17:41:23,https://github.com/programmingwithclaudio/etl_postgres_gaming,"{""proposito_principal"": ""Proyecto ETL para procesamiento de datos del rubro Gaming con visualización en Grafana"", ""dominio_aplicacion"": ""Gaming"", ""tipo_proyecto"": [""ETL Pipeline"", ""Dashboard"", ""Data Processing""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Procesamiento ETL"", ""Visualización de datos"", ""Dashboard Grafana""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""Data Analytics"", ""Business Intelligence""]}"
programmingwithclaudio/etl_postgres_grafana,Sí,Sí,,README.md,2024-01-30 08:38:20,https://github.com/programmingwithclaudio/etl_postgres_grafana,"{""proposito_principal"": ""Pipeline ETL para extraer, transformar y cargar datos en PostgreSQL con visualización en Grafana"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""Pipeline de datos"", ""ETL""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [""Grafana""], ""funcionalidades_clave"": [""Extracción de datos"", ""Transformación de datos"", ""Carga de datos"", ""Visualización de datos""], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/etl_servicar_company,No,Sí,"Informe del Proyecto ETL - Tratamiento y Carga de Datos desde Excel a SQLite Introducción El presente informe detalla el desarrollo y la implementación de un proceso ETL para la extracción, transformación y carga de datos desde un archivo Excel proporcionado por el cliente hacia una base de datos SQLite. El proyecto se ha desarrollado utilizando Python para la automatización de tareas y ha abarcado desde la preparación de datos hasta la creación y población de tablas en la base de datos. Objetivos del Proyecto El objetivo principal de este proyecto fue: Transformar los datos contenidos en el archivo Excel proporcionado por el cliente en una estructura adecuada para su posterior almacenamiento en una base de datos SQLite. Metodología Fase de Extracción Recepción del Archivo Excel: Se elaboro un archivo Excel con datos IA a ser tratados y almacenados. Fase de Transformación Preparación y Limpieza de Datos: Se cargaron los datos del archivo Excel en un DataFrame de Pandas. Se realizaron procesos de limpieza, como la conversión de tipos de datos, manejo de valores nulos e inesperados, y normalización de datos. Se aplicaron funciones de hash y transformaciones para la creación de claves únicas. (https://postimg.cc/yDByLmZB) Diseño de la Base de Datos: Se definieron las tablas y sus relaciones utilizando SQLAlchemy. Se crearon las estructuras de datos correspondientes para reflejar la estructura del modelo de datos. Fase de Carga Automatización del Proceso de Carga: Se estableció la conexión con la base de datos SQLite. Se crearon las consultas SQL necesarias para insertar los datos tratados en las respectivas tablas. Se verificó la inserción exitosa de los datos en la base de datos. Desafíos y Soluciones (https://postimg.cc/jC4L0X5Z) DBMODELO ESTRELLA Desafíos Encarados Manipulación y Tratamiento de Datos Complejos: El archivo Excel contenía datos complejos y diversas inconsistencias que requerían una limpieza detallada y un tratamiento cuidadoso. Modelado y Normalización de Datos: Definir una estructura de base de datos que refleje de manera óptima la complejidad de los datos presentes en el archivo Excel. Soluciones Aplicadas Procesos de Limpieza y Normalización: Se desarrollaron funciones específicas en Python para tratar y limpiar los datos, asegurando consistencia y coherencia en la base de datos resultante. Se aplicaron funciones de hash y transformaciones para garantizar la unicidad de las claves. Diseño Modular y Uso de Bibliotecas Específicas: Se...",README.md,2024-03-03 08:36:27,https://github.com/programmingwithclaudio/etl_servicar_company,"{""proposito_principal"": ""Transformar datos desde un archivo Excel hacia una base de datos SQLite mediante un proceso ETL"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""ETL Pipeline"", ""Data Processing""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""SQLite""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Extracción de datos desde Excel"", ""Limpieza y transformación de datos"", ""Conversión de tipos de datos"", ""Manejo de valores nulos"", ""Normalización de datos"", ""Aplicación de funciones hash"", ""Creación de claves únicas"", ""Diseño de base de datos con modelo estrella"", ""Carga automatizada a SQLite""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Data Processing"", ""ETL"", ""SQLAlchemy"", ""Pandas""]}"
programmingwithclaudio/etl_tech_ii,No,Sí,"Proyecto: Migración de Datos a Base de Datos con Pandas y SQLAlchemy Descripción del proyecto Este proyecto tiene como objetivo principal migrar datos de archivos CSV a una base de datos MySQL utilizando Pandas para la manipulación y procesamiento de datos, y SQLAlchemy para la conexión y manipulación de la base de datos. Desafíos Conexión con la base de datos: Configurar la conexión con la base de datos MySQL usando SQLAlchemy y crear las tablas necesarias. Lectura de archivos CSV: Leer múltiples archivos CSV (Clientes, Compra, Gasto, TiposDeGasto, Venta, Sucursales) usando Pandas para convertirlos en DataFrames. Manipulación y limpieza de datos: Realizar operaciones como eliminación de columnas, cambio de tipos de datos, conversión de fechas, entre otros. Migración de datos: Transferir los DataFrames a la base de datos MySQL, cada uno en su tabla correspondiente. Pasos del Proceso Configuración de la base de datos: Se establece la URL de la base de datos y se crea el motor de SQLAlchemy. Creación de modelos: Definición de modelos de datos con SQLAlchemy para las tablas de la base de datos. Lectura de archivos CSV: Uso de Pandas para leer los archivos CSV y convertirlos en DataFrames. Manipulación y limpieza de datos: Operaciones de limpieza y transformación en los DataFrames. Migración de datos: Transferencia de datos desde los DataFrames a las tablas correspondientes en la base de datos MySQL. Confirmación y cierre: Confirmación de los cambios realizados en la base de datos y cierre de la sesión. Archivos importantes en el proyecto database.py: Configuración de la conexión a la base de datos con SQLAlchemy. main.py: Lectura de archivos CSV, manipulación de datos y migración a la base de datos MySQL. models.py: Definición de modelos de datos utilizando SQLAlchemy para las tablas en la base de datos. Uso de los archivos Ejecución: Para ejecutar el proceso, se puede ejecutar el archivo , que leerá los archivos CSV, limpiará los datos y migrará la información a la base de datos MySQL. Notas adicionales Se debe tener en cuenta que se requiere una base de datos MySQL activa y configurada para que el proyecto funcione correctamente. Antes de ejecutar el proceso de migración, se debe asegurar que los archivos CSV estén presentes y sigan el formato adecuado para su correcta lectura y procesamiento. Este es un esquema básico de cómo podrías estructurar tu README.md para presentar el proyecto de migración de datos a una base de datos utilizando Pandas y...",README.md,2024-01-08 00:07:56,https://github.com/programmingwithclaudio/etl_tech_ii,"{""proposito_principal"": ""Migración de datos de archivos CSV a una base de datos MySQL utilizando Pandas y SQLAlchemy"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""ETL Pipeline"", ""Data Migration Tool""], ""tecnologias_backend"": [""SQLAlchemy""], ""tecnologias_frontend"": [], ""bases_datos"": [""MySQL""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Lectura de archivos CSV"", ""Manipulación y limpieza de datos"", ""Migración de datos a base de datos"", ""Conversión de tipos de datos"", ""Procesamiento de fechas""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""ETL"", ""Data Processing"", ""Database Migration""]}"
programmingwithclaudio/example-vite-store-js,Sí,Sí,"React + Vite This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules. Currently, two official plugins are available: @vitejs/plugin-react uses Babel for Fast Refresh @vitejs/plugin-react-swc uses SWC for Fast Refresh",README.md,2024-12-21 15:15:51,https://github.com/programmingwithclaudio/example-vite-store-js,"{""proposito_principal"": ""Template de configuración mínima para React con Vite"", ""dominio_aplicacion"": ""Desarrollo Frontend"", ""tipo_proyecto"": [""Template"", ""Frontend Web""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""React"", ""Vite""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Hot Module Replacement (HMR)"", ""Fast Refresh"", ""ESLint rules""], ""lenguajes_programacion"": [""JavaScript""], ""tags_adicionales"": [""Template"", ""Minimal Setup"", ""Development Tool""]}"
programmingwithclaudio/example-vite-store-ts,Sí,Sí,"React + TypeScript + Vite This template provides a minimal setup to get React working in Vite with HMR and some ESLint rules. Currently, two official plugins are available: @vitejs/plugin-react uses Babel for Fast Refresh @vitejs/plugin-react-swc uses SWC for Fast Refresh Expanding the ESLint configuration If you are developing a production application, we recommend updating the configuration to enable type aware lint rules: Configure the top-level property like this: Replace to or Optionally add Install eslint-plugin-react and update the config: tag v1.0.0 ""hooks"" ¿Qué es TypeScript y sus ventajas.mp4 03-guitarla-ts.zip Creando el Proyecto con TypeScript.mp4 Introducción a TypeScript - Primitive Types.mp4 Introducción a TypeScript - Types e Interfaces.mp4 Asignar Types a tus Props - Inline Type.mp4 Asignar Types a tus Props - Type Separado.mp4 Creando un Archivo de Types ¿Donde y como hacerlo.mp4 Añadiendo un Type para los elementos del Carrito.mp4 Heredar y extender un Type.Documentaci-n-Utility-Types.url Utility Types en TypeScript.mp4 Agregando el Nuevo Type a nuestro código.mp4 Creando un Type para el ID de la guitarra.mp4 Typando los Props del Header y Building del proyecto.mp4 external-links: https://www.typescriptlang.org/docs/handbook/utility-types.html tag v1.0.1 ""reducers"" 001 06-guitarla-ts-INICIO.zip 001 06-guitarla-ts-usereducer.zip 001 Primeros pasos.mp4 002 Creando el reducer y el type de acciones.mp4 003 Definiendo el State inicial.mp4 004 Escribiendo el resto de las acciones.mp4 005 Instanciando el reducer en la página principal.mp4 006 Migrando del Custom Hook hacia useReducer.mp4 007 Migrando el addToCart hacia useReducer.mp4 008 Ajustando el Código para la sintaxis de useReducer.mp4 009 Evitando registros duplicados con sintaxis de Reducer.mp4 010 Mostrando el contenido del carrito.mp4 011 Trabajando con la acción de eliminar del carrito.mp4 012 Trabajando con la acción de incrementar cantidad y RETO 02.mp4 013 SOLUCION RETO 02.mp4 014 SOLUCION RETO 03.mp4 015 Colocando el state en LocalStorage.mp4",README.md,2024-12-15 20:07:06,https://github.com/programmingwithclaudio/example-vite-store-ts,"{""proposito_principal"": ""Tienda en línea de guitarras (e-commerce) con carrito de compras y gestión de estado"", ""dominio_aplicacion"": ""E-commerce"", ""tipo_proyecto"": [""Full Stack Web"", ""Template""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""React"", ""TypeScript"", ""Vite""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Carrito de compras"", ""Gestión de estado con useReducer"", ""Persistencia en LocalStorage"", ""Validación de tipos con TypeScript"", ""Hot Module Replacement (HMR)""], ""lenguajes_programacion"": [""TypeScript"", ""JavaScript""], ""tags_adicionales"": [""Open Source"", ""Template"", ""Educational"", ""E-commerce Demo""]}"
programmingwithclaudio/exp-springboot-nro-i,No,No,Sin documentación disponible,,2025-03-31 23:47:40,https://github.com/programmingwithclaudio/exp-springboot-nro-i,"{""proposito_principal"": ""No se puede determinar por falta de documentación"", ""dominio_aplicacion"": ""No se puede determinar por falta de documentación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/fastapi-auth-system,No,Sí,"Despliegue del Proyecto: Sistema de Authenticación en Fastapi 1 Deploy local (windows 10) 1.1 Requisitos del Sistema | Componente | Versión recomendada | Notas | | --------------------- | --------------------- | ------------------------ | | Sistema Operativo | Windows 10 / Debian | Entornos compatibles | | Python | 3.12.10 | Requerido | | PostgreSQL | 17.6 | Base de datos principal | | Redis CLI | 5.0.14.1 | Módulo de caché opcional | | Docker | 29.0.0, build 3d4129b | Opcional | | Docker Compose | v2.40.3 | Opcional | 1.2 Clonación y Configuración del Entorno 1.3 Configuración de la Base de Datos Ejecutar en psql o cualquier cliente SQL compatible: Verificar detalles de conexión mediante variables de entorno en el archivo . 1.4 Ejecución del Proyecto (modo local) Acceso a la documentación interactiva: Ruta: http://localhost:8000/docs 2 Deploy mediante Docker (Kernel-Linux 5.15) (modo contenedor) 2.1 renombrar el por 2.2 Ejecución mediante Docker (modo contenedor) Validar acceso en: Ruta: http://localhost:8000/docs",README.md,2025-11-12 03:43:04,https://github.com/programmingwithclaudio/fastapi-auth-system,"{""proposito_principal"": ""Sistema de autenticación con FastAPI"", ""dominio_aplicacion"": ""Seguridad y Autenticación"", ""tipo_proyecto"": [""API REST"", ""Microservicio""], ""tecnologias_backend"": [""FastAPI""], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose""], ""funcionalidades_clave"": [""Autenticación"", ""Documentación interactiva""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Producción""]}"
programmingwithclaudio/flask-api-n8n,Sí,Sí,Objests metodos Objests metodos,README.md,2025-04-28 20:30:45,https://github.com/programmingwithclaudio/flask-api-n8n,"{""proposito_principal"": ""API Flask para integración con n8n"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""API REST"", ""Integración""], ""tecnologias_backend"": [""Flask""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": []}"
programmingwithclaudio/flask-restful-login,Sí,Sí,"flask-restful-login-example (https://travis-ci.org/joemccann/dillinger) INSTALLATION Python 3 is required. There are ways to send requests to server. Postman, Insomnia, cURL, httpie and curl are simple and useful tools to send requests. I mostly prefer httpie and curl. Their usage can be seen below. Pull project and install requirements to virtual environment (https://pypi.org/project/virtualenv/()). Then run. For requests using httpie: https://httpie.io/() For requests using curl: https://curl.haxx.se/download.html() Example user, admin and super admin users are created in database initializer class. You can use these users to login, logout and data handlers. For register handler, use new user information, otherwise returns already exist user. | Test Users | Email Address | Password | | ------------- |:-------------:| -----:| | User | testemail@example.com | testpassword | | Admin | adminemail@example.com | adminpassword | | Super Admin | saemail@example.com | sapassword | Register: HTTPIE Request: Curl Request: Login: HTTPIE Request: Curl Request: Response: Got access token and refresh token Logout: HTTPIE Request: Curl Request: Reset Password: HTTPIE Request: Curl Request: There are some example routes in UserHandlers file. These handlers mostly return only text. To use them: Example routes that require authentication Route addresses according to user privileges | User Type | Route Address | | ------------- |:-------------:| | User | /datauser | | Admin | /dataadmin | | Super Admin | /datasuperadmin | HTTPIE Request: Curl Request: Super admin requiring authentication extra example handler, list users This handler searches username, email or creation dates (range) in users table and returns information these users to super admin. HTTPIE Request: Curl Request: License MIT Free Software, Hell Yeah",README.md,2024-09-30 23:43:49,https://github.com/programmingwithclaudio/flask-restful-login,"{""proposito_principal"": ""API REST para sistema de autenticación y autorización de usuarios con roles"", ""dominio_aplicacion"": ""Seguridad y Autenticación"", ""tipo_proyecto"": [""API REST"", ""Backend Service""], ""tecnologias_backend"": [""Flask""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Autenticación JWT"", ""Sistema de roles (User, Admin, Super Admin)"", ""Registro de usuarios"", ""Login/Logout"", ""Reset de contraseñas"", ""Autorización basada en privilegios""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""MIT License"", ""Authentication API""]}"
programmingwithclaudio/flow_pry_auren,Sí,Sí,Instalar con Proxy,README.md,2025-10-28 13:43:12,https://github.com/programmingwithclaudio/flow_pry_auren,"{""proposito_principal"": ""Instalar con Proxy"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""CLI Tool""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Instalación con configuración de proxy""], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/full_streamlit,Sí,Sí,"Use Streamlit to create a personal website Create a personal website using Python and the Streamlit library. This website will be able to serve as a place for you to share your thoughts, projects, and anything else you want. Requirements",README.md,2024-05-06 20:03:43,https://github.com/programmingwithclaudio/full_streamlit,"{""proposito_principal"": ""Crear un sitio web personal usando Python y la librería Streamlit para compartir pensamientos, proyectos y contenido personal"", ""dominio_aplicacion"": ""Desarrollo Web Personal"", ""tipo_proyecto"": [""Full Stack Web"", ""Dashboard""], ""tecnologias_backend"": [""Streamlit""], ""tecnologias_frontend"": [""Streamlit""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Creación de sitios web personales"", ""Compartir contenido personal"", ""Interfaz web interactiva""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Template"", ""Educativo""]}"
programmingwithclaudio/generate_dbf_sql,Sí,Sí,"Proyecto para BASE DE DATOS AVANCE 30% Este proyecto consiste en transformar datos.DBF a .SQL, especialmente para cargarlos a mi gestor mariaDB, que es la base de datos con que trabajo actualmente. Una vez concluya sera un rediseño de una base de datos Empresarial compleja.",README.md,2024-02-08 16:53:07,https://github.com/programmingwithclaudio/generate_dbf_sql,"{""proposito_principal"": ""Transformar datos .DBF a .SQL para cargarlos a MariaDB"", ""dominio_aplicacion"": ""Base de datos empresarial"", ""tipo_proyecto"": [""Herramienta de conversión de datos"", ""Pipeline de datos""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""MariaDB""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Conversión DBF a SQL"", ""Carga de datos a MariaDB"", ""Rediseño de base de datos empresarial""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""Conversión de formatos"", ""Migración de datos""]}"
programmingwithclaudio/hadoop_cluster,Sí,Sí,"Proyecto: Análisis de Fraudes en Comunicaciones Corporativas con el Conjunto de Datos de Enron Email: Conjunto de Datos: Enron Email Dataset Descripción: En este proyecto, se podrían analizar los correos electrónicos de Enron para identificar patrones sospechosos y detectar posibles fraudes. Esto implica técnicas de minería de texto y análisis de anomalías. Datasets Enron",README.md,2024-01-27 03:34:51,https://github.com/programmingwithclaudio/hadoop_cluster,"{""proposito_principal"": ""Análisis de fraudes en comunicaciones corporativas usando el conjunto de datos de Enron Email"", ""dominio_aplicacion"": ""Finanzas"", ""tipo_proyecto"": [""Data Science"", ""Análisis de datos""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""Minería de texto"", ""Análisis de anomalías""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Identificación de patrones sospechosos"", ""Detección de fraudes"", ""Análisis de correos electrónicos""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Enron Dataset"", ""Análisis forense"", ""Comunicaciones corporativas""]}"
programmingwithclaudio/ia-tools-desktop,Sí,No,Sin documentación disponible,,2024-10-08 21:34:22,https://github.com/programmingwithclaudio/ia-tools-desktop,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/ia_chat_hugginsface,No,Sí,"IA Chat App with Hugging Face (https://github.com/programmingwithclaudio/iachatlangchain) (https://github.com/programmingwithclaudio/iachatlangchain) Aplicación de chat inteligente con capacidades de IA utilizando LangChain, base para proyectos posteriores. 🛠️ Requerimientos Técnicos Bases de Datos: 🍃 MongoDB (Almacenamiento principal) 🧠 Redis (Cache y sesiones) Dependencias Clave: menloltd/cortex (Procesamiento de IA) LangChain (Flujos conversacionales) Next.js (Frontend) 📌 Plan de Implementación | Etapa | Estado | Detalle | | --------------------------- | -------------- | ------------------------------ | | 1. Sistema de Autenticación | ✅ Completado | Login con JWT y OAuth 2.0 | | 2. Núcleo de IA | 🚧 En Progreso | Integración LangChain + Cortex | | 3. Diseño de Interfaces | 🚧 En Progreso | Sistema de Chat Responsivo | | 4. Deployment | 🗓️ Pendiente | Configuración Vercel | 📅 Cronograma 🔍 Referencias Clave Documentación LangChain Arquitectura Cortex cortex-plataforma-de-la-ia-albertcoronado 🚀 Cómo Contribuir Clona el repositorio Instala dependencias: Configura variables de entorno (.env) Inicia servidores de , y la : Ejecuta la app: Nota: Images en servidores : utils-base-de-datos - Recomendaciones: Si sabes configura GPU, pero si estas empezando a integrar servicios de ia en el desarrollo utiliza el procesador y la RAM por defecto como en el .",README.md,2025-03-12 17:17:49,https://github.com/programmingwithclaudio/ia_chat_hugginsface,"{""proposito_principal"": ""Aplicación de chat inteligente con capacidades de IA utilizando LangChain"", ""dominio_aplicacion"": ""Chatbots y Asistentes Virtuales"", ""tipo_proyecto"": [""Full Stack Web"", ""Chat Application""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [""MongoDB"", ""Redis""], ""ml_ia"": [""LangChain"", ""cortex (menloltd/cortex)""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Autenticación JWT"", ""Login con OAuth 2.0"", ""Sistema de Chat Responsivo""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Base para proyectos posteriores"", ""En desarrollo""]}"
programmingwithclaudio/ia_cuentas_por_cobrar,Sí,Sí,"Presentación del Proyecto de Inteligencia Artificial para Gestión de Cuentas por Cobrar Resumen del Proyecto El proyecto se enfoca en la aplicación de técnicas de Inteligencia Artificial para optimizar la gestión de cuentas por cobrar en una organización. Utilizamos técnicas de Procesamiento de Datos y Aprendizaje Automático para analizar, clasificar y predecir el comportamiento de las cuentas por cobrar, identificando patrones y recomendando acciones específicas para mejorar la eficiencia y reducir los riesgos financieros, y además se combinan técnicas de análisis financiero y de acción para las respectivas cuentas. Resumen de Datos y Procesamiento Datos de Entrada: El conjunto de datos proporciona un análisis detallado de productos, montos, vendedores, prioridades y más, extraídos del Libro Diario, Registro de Ventas, Datos de Clientes y Detalles de la Compañía. Las características principales incluyen cantidad, precio, montos, prioridades, entre otros aspectos que conforman el modelo para el análisis de cuentas por cobrar. Se considera la posibilidad de aplicar metodologías como Six Sigma y otras para mejorar la eficiencia operativa y la gestión de riesgos en el proceso de cobranza. Preprocesamiento: Limpieza de datos para manejar valores faltantes y convertir datos categóricos en variables numéricas utilizables. Se realizó una revisión exhaustiva para garantizar la integridad de los datos y su aptitud para el modelado. Análisis Exploratorio de Datos (EDA) Visualizaciones de Datos: Exploración detallada de las relaciones entre las variables clave. Análisis de distribuciones, correlaciones y tendencias. Descubrimientos: Identificación de factores importantes para la gestión de cuentas por cobrar. Destacamos las relaciones clave que influyen en el comportamiento de cobranza. Modelado y Evaluación Selección de Modelo: Utilizamos diferentes algoritmos de Aprendizaje Automático (como Decision Trees) para predecir el comportamiento de las cuentas por cobrar. Ajuste de hiperparámetros para mejorar la precisión y la generalización. (https://postimg.cc/rdB4sXq2) Resultados: Obtuvimos una precisión promedio del 98.12% utilizando DecisionTreeClassifier con una profundidad máxima de 3 y un número mínimo de muestras en las hojas de 32. Conclusiones y Recomendaciones Conclusiones: Se logró un alto nivel de precisión en la predicción del comportamiento de las cuentas por cobrar. Identificamos patrones clave que pueden ayudar a mejorar la gestión financiera y reducir...",README.md,2024-05-08 22:58:28,https://github.com/programmingwithclaudio/ia_cuentas_por_cobrar,"{""proposito_principal"": ""Aplicación de técnicas de Inteligencia Artificial para optimizar la gestión de cuentas por cobrar mediante análisis, clasificación y predicción del comportamiento financiero"", ""dominio_aplicacion"": ""Finanzas"", ""tipo_proyecto"": [""Data Science"", ""Machine Learning""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""Decision Trees"", ""DecisionTreeClassifier""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Análisis de cuentas por cobrar"", ""Clasificación de comportamiento financiero"", ""Predicción de riesgos"", ""Procesamiento de datos financieros"", ""Análisis exploratorio de datos (EDA)"", ""Limpieza de datos"", ""Preprocesamiento de datos""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Six Sigma"", ""Análisis financiero"", ""Gestión de riesgos"", ""Procesamiento de datos""]}"
programmingwithclaudio/ia_pygame,Sí,Sí,"Proyecto de IA, gymlibrary.dev/environments/ gymnasium.farama. Obtener los entornos ia de gymnasium environments/atari/ Acrobot-v1 Ant-v2 Ant-v3 Ant-v4 BipedalWalker-v3 BipedalWalkerHardcore-v3 Blackjack-v1 CarRacing-v2 CartPole-v0 CartPole-v1 CliffWalking-v0 FrozenLake-v1 FrozenLake8x8-v1 GymV21Environment-v0 GymV26Environment-v0 HalfCheetah-v2 HalfCheetah-v3 HalfCheetah-v4 Hopper-v2 Hopper-v3 Hopper-v4 Humanoid-v2 Humanoid-v3 Humanoid-v4 HumanoidStandup-v2 HumanoidStandup-v4 InvertedDoublePendulum-v2 InvertedDoublePendulum-v4 InvertedPendulum-v2 InvertedPendulum-v4 LunarLander-v2 LunarLanderContinuous-v2 MountainCar-v0 MountainCarContinuous-v0 Pendulum-v1 Pusher-v2 Pusher-v4 Reacher-v2 Reacher-v4 Swimmer-v2 Swimmer-v3 Swimmer-v4 Taxi-v3 Walker2d-v2 Walker2d-v3 Walker2d-v4 phys2d/CartPole-v0 phys2d/CartPole-v1 phys2d/Pendulum-v0 tabular/Blackjack-v0 tabular/CliffWalking-v0 import torch t = torch.Tensor(3,3) print(t) Referencias: Docker. (2024). Docker.com. https://hub.docker.com/u/bde2020 George, N. (2018). All Lending Club loan data (Version 3) Data set. Kaggle. https://www.kaggle.com/wordsforthewise/lending-club",README.md,2024-02-11 05:08:18,https://github.com/programmingwithclaudio/ia_pygame,"{""proposito_principal"": ""Implementación de algoritmos de IA para entrenar agentes en entornos de Gymnasium/OpenAI Gym"", ""dominio_aplicacion"": ""Inteligencia Artificial"", ""tipo_proyecto"": [""Librería"", ""Machine Learning""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""PyTorch"", ""Gymnasium"", ""OpenAI Gym""], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Entrenamiento de agentes de IA"", ""Simulación de entornos físicos"", ""Reinforcement Learning""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Experimental"", ""Research""]}"
programmingwithclaudio/ia_retail_pyspark,No,Sí,"Desarrollo del Modelo en PySpark con FastAPI y Docker Resumen Este proyecto busca implementar un modelo de regresión lineal utilizando PySpark, expuesto a través de un endpoint con FastAPI, y encapsulado en un contenedor Docker. El objetivo es brindar una solución escalable y fácilmente implementable. Datos de la Data El conjunto de datos ""Online Retail II"" contiene transacciones de comercio minorista en línea durante dos años. Con variables como InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID y Country, es un recurso valioso para tareas de clasificación, regresión y agrupamiento. Desafíos y Soluciones Configuración del Entorno PySpark Desafío: Configurar un entorno PySpark para el desarrollo del modelo. Solución: Implementación de un entorno Docker basado en Jupyter con soporte para PySpark. Desarrollo del Modelo Desafío: Diseñar un modelo de regresión lineal eficiente. Solución: Utilización del módulo para implementar un modelo robusto. Manejo de Archivos Parquet Desafío: Cargar y procesar datos desde archivos Parquet. Solución: Aplicación de funciones PySpark para la lectura eficiente de archivos Parquet. Configuración de Docker Desafío: Crear un contenedor Docker que incluya todas las dependencias necesarias. Solución: Desarrollo de un Dockerfile basado en la imagen oficial de Jupyter con soporte para PySpark. Exposición del Endpoint con FastAPI Desafío: Implementar un servidor FastAPI para gestionar solicitudes HTTP y realizar predicciones. Solución: Desarrollo de un endpoint que utiliza el modelo PySpark para generar predicciones. (https://postimg.cc/zHBt7czr) Instrucciones de Ejecución Construir la imagen Docker: Ejecutar el contenedor: Acceder a la documentación del API FastAPI: http://localhost:8000/docs Realizar solicitudes POST al endpoint para obtener predicciones. Referencia del Conjunto de Datos Chen, Daqing. (2019). Online Retail II. UCI Machine Learning Repository. DOI. Este conjunto de datos está bajo la licencia Creative Commons Attribution 4.0 International (CC BY 4.0), permitiendo compartir y adaptar los datos con el crédito adecuado.",README.md,2024-01-13 03:03:51,https://github.com/programmingwithclaudio/ia_retail_pyspark,"{""proposito_principal"": ""Implementar un modelo de regresión lineal utilizando PySpark expuesto a través de un endpoint con FastAPI y encapsulado en un contenedor Docker"", ""dominio_aplicacion"": ""Retail/Comercio minorista"", ""tipo_proyecto"": [""API REST"", ""Modelo de Machine Learning"", ""Contenedor Docker""], ""tecnologias_backend"": [""FastAPI"", ""PySpark""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""PySpark ML"", ""Regresión lineal""], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Endpoint de predicción"", ""Procesamiento de datos Parquet"", ""Modelo de regresión lineal"", ""Servidor HTTP FastAPI""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Jupyter"", ""Escalable"", ""Online Retail II dataset""]}"
programmingwithclaudio/kali-hacking-example,Sí,Sí,"Directorio de C:\Users\crow\Videos\Universidad Hacking. Todo En Ciberseguridad 03/03/2022 01:14 . 03/03/2022 01:14 .. 03/03/2022 00:46 1. Sobre El Curso Y Consejos Para Realizarlo 03/03/2022 00:51 10. Parte 2 - Estructuras De Control 03/03/2022 00:52 11. Parte 2 - Tuplas, Diccionarios, Conjuntos, Pilas Y Colas 03/03/2022 00:53 12. Parte 2 - Entradas Por Teclado Y Salidas Por Pantalla 03/03/2022 01:05 13. Parte 2 - Funciones 03/03/2022 01:05 14. Parte 2 - Errores Y Excepciones 03/03/2022 01:06 15. Parte 2 - POO (Programación Orientada A Objetos) 03/03/2022 01:06 16. Parte 2 - Demostración De Que Se Puede Hacer Con Hacking Con Python 03/03/2022 01:07 17. Parte 3 - Práctica 03/03/2022 01:07 18. Parte 3 - Reconocimiento Del Objetivo. Herramientas Varias 03/03/2022 01:08 19. Parte 4 - OWASP 03/03/2022 00:46 2. Parte 1 - Conocimientos Que Debes Tener Para Empezar En Seguridad Informática 03/03/2022 01:08 20. Parte 5 - Metasploit 03/03/2022 01:08 21. Parte 5 - Hacking Con Metasploit 03/03/2022 01:09 22. Parte 6 - Análisis De Malware 03/03/2022 01:10 23. Parte 6 - El Arte Del Ocultamiento De Malware 03/03/2022 01:10 24. Introducción A La Parte 7 03/03/2022 01:11 25. Parte 7 - Seguridad Informática En Nuestro Celular 03/03/2022 01:11 26. Parte 7 - Herramientas De Seguridad En Nuestro Celular 03/03/2022 01:11 27. Parte 7 - Owasp Mobile 03/03/2022 01:12 28. Parte 8 - Privacidad 03/03/2022 01:12 29. Plus De Clases 03/03/2022 00:47 3. Parte 1 - Redes 03/03/2022 01:12 30. Bonus Extra 03/03/2022 00:47 4. Parte 1 - Conceptos Fundamentales Para Seguridad Informática 03/03/2022 00:48 5. Parte 1 - Preparamos Y Conocemos Nuestro Escenario De Trabajo 03/03/2022 00:48 6. Parte 2 - Programación 03/03/2022 00:49 7. Parte 2 - Aprendemos Python 03/03/2022 00:50 8. Parte 2 - Primeros Pasos 03/03/2022 00:50 9. Parte 2 - Operadores Relacionales, Lógicos Y Asignación. Expresiones Anidadas 03/03/2022 01:14 577,808 Screenshot1.png 1 archivos 577,808 bytes 32 dirs 164,516,200,448 bytes libres C:\Users\crow\Videos\Universidad Hacking. Todo En Ciberseguridad Directorio de C:\Users\crow\Videos\Universidad Hacking. Todo En Ciberseguridad\3. Parte 1 - Redes 03/03/2022 00:47 . 03/03/2022 00:47 .. 21/10/2020 20:10 143,302,222 1. Que es una red y una ip..mp4 se conforma con mas de un dispositva conectados entre si la ip individualizar el dispositivo en la red Adaptador de Ethernet Ethernet: Sufijo DNS específico para la conexión. . : Vínculo: dirección IPv6 local. . . :...",README.md,2024-10-29 09:19:11,https://github.com/programmingwithclaudio/kali-hacking-example,"{""proposito_principal"": ""Curso educativo sobre hacking ético y ciberseguridad utilizando Kali Linux"", ""dominio_aplicacion"": ""Ciberseguridad"", ""tipo_proyecto"": [""Material Educativo"", ""Curso""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Programación en Python"", ""Reconocimiento de objetivos"", ""Uso de herramientas de hacking"", ""Análisis de malware"", ""Metasploit"", ""OWASP"", ""Seguridad móvil""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Kali Linux"", ""Hacking Ético"", ""Material de Estudio"", ""Videos Educativos""]}"
programmingwithclaudio/llm_engineering,Sí,Sí,"Ingeniería de LLM - Domina el mundo de la IA y los LLMs Tu viaje de 8 semanas hacia el dominio completo de estos temas comienza hoy Me alegro mucho de que me acompañes en este camino. Vamos a construir proyectos inmensamente satisfactorios en las próximas semanas. Algunos serán fáciles, otros supondrán un reto, ¡y muchos te ASOMBRARÁN Los proyectos se basan unos en otros para que desarrolles una experiencia cada vez más profunda cada semana. Una cosa es segura: te divertirás mucho por el camino. Una nota antes de empezar Estoy aquí para ayudarte a tener más éxito en tu aprendizaje. Si te encuentras con algún problema, o si tienes alguna idea sobre cómo puedo mejorar el curso, por favor, ponte en contacto conmigo en la plataforma o enviándome un correo electrónico directamente (juangabriel@frogames.es). Siempre es bueno conectar con la gente en LinkedIn para construir la comunidad - me encontrarás aquí: https://www.linkedin.com/in/juan-gabriel-gomila-salas/ Instrucciones de Gratificación Instantánea para la Semana 1, Día 1 ¡Comenzaremos el curso instalando Ollama para que puedas ver los resultados de inmediato Descarga e instala Ollama desde https://ollama.com En una PC, inicia un Símbolo del sistema/PowerShell (Presiona Win + R, escribe y presiona Enter). En una Mac, inicia una Terminal (Aplicaciones Utilidades Terminal). Ejecuta o, para máquinas más pequeñas, prueba Si esto no funciona, es posible que debas ejecutar en otro PowerShell (Windows) o Terminal (Mac) e intentar el paso 3 nuevamente Y si eso no funciona en tu equipo, lo he configurado en la nube. Esto está en Google Colab, que necesitará que tengas una cuenta de Google para iniciar sesión, pero es gratis: https://colab.research.google.com/drive/1i5hHBpd424gNuO0T8AsbLDBR2toRh8K?usp=sharing Si tienes algún problema, ¡contacta conmigo A continuación, instrucciones de configuración Después de realizar el proyecto rápido de Ollama y de presentarme y presentar el curso, nos ponemos a trabajar con la configuración completa del entorno. Espero haber hecho un buen trabajo para que estas guías sean infalibles, pero comuníquese conmigo de inmediato si encuentra obstáculos: Usuarios de PC, podéis seguir las instrucciones en SETUP-PC.md Usuarios de Mac, podéis seguir las instrucciones en SETUP-mac.md Usuarios de Linux, ¡las instrucciones para Mac deberían ser lo suficientemente precisas Un punto importante sobre los costes de las API Durante el curso, te sugeriré que pruebes los principales modelos a la...",README.md,2025-06-23 10:00:44,https://github.com/programmingwithclaudio/llm_engineering,"{""proposito_principal"": ""Curso educativo de 8 semanas para dominar ingeniería de LLMs y desarrollo de proyectos de IA"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Curso Educativo"", ""Tutoriales"", ""Proyectos de Aprendizaje""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""Ollama"", ""Modelos LLM""], ""devops_cloud"": [""Google Colab""], ""funcionalidades_clave"": [""Instalación de Ollama"", ""Configuración de entorno"", ""Proyectos prácticos"", ""Guías de setup para PC/Mac/Linux""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Educativo"", ""Curso"", ""Tutorial"", ""Aprendizaje"", ""Setup""]}"
programmingwithclaudio/llm_ia_inmobiliario,Sí,Sí,,README.md,2024-02-17 04:35:37,https://github.com/programmingwithclaudio/llm_ia_inmobiliario,"{""proposito_principal"": ""No se puede determinar - documentación insuficiente"", ""dominio_aplicacion"": ""No se puede determinar - documentación insuficiente"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/load_minio_bucked,Sí,Sí,"📌 Flujo de Persistencia de Datos MinIO almacena archivos en Se monta como volumen persistente en el contenedor . El cliente S3FS monta MinIO como un sistema de archivos en Cualquier archivo escrito en se almacenará en MinIO. Python interactúa con MinIO vía SDK Puede leer, escribir y listar archivos en MinIO. Todos los datos persisten después de un reinicio Gracias a los volúmenes montados, ni los archivos ni la configuración se pierden. 🚀 Cómo Ejecutar 1️⃣ Clonar el repositorio o copiar los archivos a un directorio. 2️⃣ Crear el archivo con las credenciales. 3️⃣ Ejecutar: 4️⃣ Verificar logs de cada servicio con: 5️⃣ Acceder a la consola web de MinIO: URL: http://localhost:9090 Usuario: Contraseña: https://github.com/AhmetFurkanDEMIR/airflow-spark-kafka-example/blob/main/docker/airflow/start-airflow.sh http://localhost:9001/login http://localhost:8080/",README.md,2025-03-15 18:05:51,https://github.com/programmingwithclaudio/load_minio_bucked,"{""proposito_principal"": ""Sistema de persistencia de datos que monta MinIO como sistema de archivos usando S3FS para almacenamiento distribuido"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""Infrastructure"", ""Data Persistence""], ""tecnologias_backend"": [""Python""], ""tecnologias_frontend"": [], ""bases_datos"": [""MinIO""], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""S3FS"", ""MinIO""], ""funcionalidades_clave"": [""Montaje de MinIO como sistema de archivos"", ""Persistencia de datos con volúmenes"", ""Interacción con MinIO vía SDK"", ""Almacenamiento distribuido""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Storage Solution""]}"
programmingwithclaudio/Login-WebApp-Mariadb,Sí,Sí,Incio de Login-WebApp.Mariadb Organization https://github.com/programmingwithclaudio/notesdev-skill-dev/blob/main/Linux/tomcat-gpt.md project https://github.com/programmingwithclaudio/notesdev-skill-dev/blob/main/Windows/LoginWebAppMariadb.md Modelo Basededatos SQL mvn clean package,README.md,2024-12-02 02:35:41,https://github.com/programmingwithclaudio/Login-WebApp-Mariadb,"{""proposito_principal"": ""Sistema de autenticación web con base de datos MariaDB"", ""dominio_aplicacion"": ""Autenticación y seguridad web"", ""tipo_proyecto"": [""Full Stack Web"", ""Sistema de autenticación""], ""tecnologias_backend"": [""Java"", ""Maven""], ""tecnologias_frontend"": [], ""bases_datos"": [""MariaDB""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Autenticación de usuarios"", ""Gestión de sesiones"", ""Modelo de base de datos SQL""], ""lenguajes_programacion"": [""Java"", ""SQL""], ""tags_adicionales"": [""Tomcat"", ""Maven build""]}"
programmingwithclaudio/ls_model_ia,Sí,No,Sin documentación disponible,,2025-06-24 10:48:09,https://github.com/programmingwithclaudio/ls_model_ia,"{""proposito_principal"": ""No se puede determinar el propósito del proyecto debido a falta de documentación"", ""dominio_aplicacion"": ""No se puede determinar el dominio de aplicación debido a falta de documentación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/MLops_pypark_steam,Sí,Sí,"#df = spark.read.format(""com.databricks.spark.csv"").option(""header"", ""true"").load(""../datasets/raw/steamreviews.json.gz"") Get-Content steamnew.json -TotalCount 10 # Muestra las primeras 10 líneas australianuserreviews.json Get-Content australianuserreviews.json -TotalCount 10 El error que estás enfrentando es una restricción introducida en Spark 2.3. Ahora, las consultas directas sobre columnas internas de registros corruptos () en archivos JSON/CSV están deshabilitadas. Esto se debe a que las columnas que hacen referencia únicamente al registro interno corrupto no son tratadas como columnas regulares en Spark. Esto significa que no puedes realizar operaciones directas sobre en el momento de la carga. La sugerencia es almacenar o guardar los resultados analizados y luego realizar la consulta. Aquí hay un ejemplo de cómo podrías abordarlo: Aquí, almacenamos en caché () el DataFrame después de cargar los datos y luego realizamos la consulta deseada. La idea es evitar realizar la consulta directamente durante la carga del DataFrame. Este enfoque debería ayudarte a eludir la restricción y realizar la consulta que necesitas. En la configuración para Hadoop que proporcionaste: La línea indica que el directorio de configuración de Hadoop se encuentra en . Sin embargo, es un poco inusual que el directorio de configuración de Hadoop esté dentro de un directorio llamado ""jars"". Lo común es que el directorio de configuración de Hadoop contenga archivos de configuración, como , , etc. Dado que estás utilizando Spark sin Hadoop directamente y mencionaste que tienes problemas con la configuración, podrías simplificar esta sección eliminando la línea que establece . Quedaría así: En cuanto a los archivos JAR que mencionaste, como , , etc., si no estás utilizando Hadoop directamente en tu aplicación Spark, puedes ignorarlos. Spark maneja la interacción con el sistema de archivos subyacente, y no necesitas preocuparte por estas bibliotecas de Hadoop en el contexto de Spark independiente. Si después de estas modificaciones aún enfrentas problemas, por favor, proporciona más detalles sobre los errores específicos que estás experimentando para que pueda ayudarte de manera más precisa.",README.md,2024-02-26 05:55:15,https://github.com/programmingwithclaudio/MLops_pypark_steam,"{""proposito_principal"": ""Procesamiento y análisis de datos de reseñas de Steam usando PySpark"", ""dominio_aplicacion"": ""Data Science"", ""tipo_proyecto"": [""Pipeline de datos""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Lectura de archivos CSV/JSON"", ""Procesamiento de datos con Spark"", ""Manejo de registros corruptos en Spark""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Spark"", ""PySpark"", ""Data Processing"", ""Steam Reviews""]}"
programmingwithclaudio/ML_Cluster_Siniestros_CABA,No,Sí,"Análisis de Siniestros Viales en la región de CABA en los periodos 2016 y 2021 Descripción Este proyecto tiene como objetivo analizar los datos de siniestros viales fatales (homicidios) en la Ciudad de Buenos Aires, Argentina. Los datos provienen de diferentes fuentes gubernamentales y contienen información sobre las víctimas, las ubicaciones geográficas, las características de los siniestros y los datos demográficos de las comunas de la ciudad. Contenido Datasets y Recursos de Análisis . Código fuente en Python para la descarga, preprocesamiento, análisis y visualización de datos . Modelo sqlalchemy en el proyecto migrados a PosgreSQL . Dashboard del proyecto KPI 10% reducción Siniestros Viales Semestre Anterior y KPI 7% reducción Siniestros Viales tipo Moto : Tableau Siniestros Fatales Análisis Exploratorio de Datos (EDA) Se han realizado análisis descriptivos de cada tabla, destacando estadísticas como la moda, mediana y media para variables clave. El análisis de correlación en el modelo de ""Siniestros Fatales"" revela patrones significativos. La correlación muy fuerte (0.98) entre ""anio"" e ""id"" indica una relación temporal positiva. La correlación moderada negativa (-0.68) entre ""comunax"" y ""longitud"" sugiere una posible relación inversa entre la comuna y la longitud geográfica. En términos del impacto en la población total (""totalpob""), las correlaciones positivas de 0.31 y 0.45 con ""area"" y ""perimetro"", respectivamente, sugieren que un mayor área y perímetro están asociados con una población total más grande. Detección de Valores Anómalos y Nulos Se han identificado y registrado valores faltantes en los DataFrames, y se ha generado un resumen de la presencia de valores . El análisis de valores faltantes revela que la columna ""Altura"" en el DataFrame ""homicidios"" tiene el mayor porcentaje de valores faltantes (81.47%). La columna ""FECHAFALLECIMIENTO"" en ""victimah"" no tiene valores nulos, pero tiene un 9.48% de valores raros. Otros porcentajes de valores faltantes oscilan entre 0.14% y 9.48%. Estos resultados señalan áreas críticas para la imputación o eliminación de datos. Interpretación de Modelos Relacionales (DBML) Se ha proporcionado un modelo relacional en DBML para representar las relaciones entre las tablas homicidiosh, victimasl, comunasl, y censol. Cada tabla tiene sus columnas definidas con tipos de datos adecuados y relaciones clave para facilitar futuras consultas. Análisis de clústeres: En cuanto al análisis de clústeres, el código aplica...",README.md,2024-03-13 09:32:29,https://github.com/programmingwithclaudio/ML_Cluster_Siniestros_CABA,"{""proposito_principal"": ""Análisis de datos de siniestros viales fatales en la Ciudad de Buenos Aires para identificar patrones y tendencias"", ""dominio_aplicacion"": ""Seguridad vial / Análisis de datos gubernamentales"", ""tipo_proyecto"": [""Data Science"", ""Análisis Exploratorio de Datos (EDA)"", ""Dashboard"", ""Análisis de clústeres""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL""], ""ml_ia"": [""Análisis de clústeres""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Análisis descriptivo de datos"", ""Detección de valores anómalos y nulos"", ""Análisis de correlación"", ""Modelo relacional DBML"", ""Visualización de datos"", ""Dashboard con KPIs""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Datos gubernamentales"", ""Análisis geográfico"", ""Tableau""]}"
programmingwithclaudio/mod_integrador,Sí,Sí,"Carreras en Henry Descripción General Un programa de estudios intensivo y online que te conectará con el mundo laboral, ofreciendo diversas carreras para desarrollar habilidades técnicas y adentrarte en el ámbito tecnológico. Carreras Disponibles Desarrollo Web Full Stack Aprende a desarrollar páginas y aplicaciones web para conseguir un trabajo en el mundo de la tecnología. Data Science Trabaja sobre todo el ciclo de vida del dato y desarrolla tu perfil como Data Analyst, Data Engineer y Data Scientist. Data Analytics Da tu primer paso en el mundo de los datos. Aprende a analizar grandes volúmenes de datos y transfórmalos en información valiosa para el negocio. Cursos Introductorios Inicia tu camino con nuestros cursos diseñados para principiantes. Introducción a la Programación Introducción a Data Science Introducción a Data Analytics Introducción a Prompt Engineering Cursos Upskilling Amplía tus conocimientos y da tu próximo paso en tecnología. ¿Qué herramientas y tecnologías se utilizan en Data Analytics? En Data Analytics se utilizan diversas herramientas y técnicas para analizar grandes conjuntos de datos y extraer información valiosa. Algunas de las tecnologías incluyen: | Lenguajes de Programación | Herramientas de Visualización | Plataformas de Análisis de Datos | | ------------------------- | ---------------------------- | --------------------------------- | | Python | Power BI | Apache Hadoop | | SQL | Tableau | Apache Spark | También se emplean técnicas estadísticas como el análisis de regresión y el análisis de series de tiempo para descubrir patrones y tendencias. ¿Data Analytics requiere programación? Sí, se requieren habilidades de programación para manipular grandes conjuntos de datos y analizarlos de manera efectiva. Los analistas de datos suelen utilizar lenguajes de programación como Python y SQL, así como herramientas de análisis de datos como Power BI. ¿Es difícil Data Analytics? La dificultad en Data Analytics depende del nivel de habilidad técnica y experiencia del analista de datos. Aunque puede ser complejo, la Carrera de Data Analytics de Henry ofrece un programa de 3 o 6 meses (dependiendo de la modalidad) para aprender todo lo necesario y facilitar el proceso. ¿Qué requisitos se necesitan para ser Data Analyst? Solo necesitas una computadora y acceso a Internet, junto con pasión por conseguir un trabajo en tecnología. ¿Cuánto dura el programa? Programa Full-Time: 3 meses (40 hs por semana, 15 hs en vivo + tiempo asincrónico de...",README.md,2024-02-22 23:51:27,https://github.com/programmingwithclaudio/mod_integrador,"{""proposito_principal"": ""Programa educativo de carreras tecnológicas intensivas y online"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Plataforma educativa"", ""Sitio web informativo""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Información de carreras"", ""Descripción de cursos"", ""Detalles de programas educativos""], ""lenguajes_programacion"": [""Python"", ""SQL""], ""tags_adicionales"": [""Educación tecnológica"", ""Bootcamp"", ""Cursos online""]}"
programmingwithclaudio/MVP_ML_TransporteNY,No,Sí,"MLOPS de Predicción de Precios en el Servicio de Taxi en el Región de New York en base a los periodos de 2022 al 2023. Introducción y Contexto de la Ciudad de Nueva York Nueva York ha experimentado un crecimiento acelerado en las últimas décadas, con más de 8 millones de habitantes en su zona urbana y más de 22 millones en la zona metropolitana. Esto ha resultado en una densidad poblacional de 10,756 habitantes por kilómetro cuadrado. Dada esta densidad y el constante flujo de personas, los desafíos en términos de transporte son significativos. El tiempo promedio de viaje para llegar al trabajo supera los 40 minutos, y las opciones de transporte incluyen el metro, autobuses, bicicletas y taxis, todos regulados por la Comisión de Taxis y Limusinas (TLC) y el Departamento de Transporte de la Ciudad de Nueva York (DOT). Una empresa de servicios de transporte de pasajeros, actualmente operando en el sector de micros de media y larga distancia, está interesada en invertir en el sector de transporte de pasajeros con automóviles en Nueva York. Con una visión de un futuro menos contaminado y en línea con las tendencias de mercado actuales, la empresa busca investigar la rentabilidad del negocio y verificar la relación entre este tipo de transporte y la calidad del aire, entre otras externalidades. Esto implica estudiar la viabilidad de implementar vehículos eléctricos en su flota, ya sea en su totalidad o en parte. Dado que esta sería una nueva unidad de negocio, se propone realizar un análisis preliminar del movimiento de los taxis en la ciudad de Nueva York para tener un marco de referencia y tomar decisiones fundamentadas. Objetivos del Proyecto Recopilar, depurar y disponibilizar información relevante para el análisis. Realizar un análisis exploratorio de datos para identificar patrones y tendencias. Crear modelos de Machine Learning para predecir la demanda de vehículos en diferentes sectores de Nueva York. Desarrollar una infraestructura de MLOps mixta utilizando módulos y scripts locales, así como automatización en la nube. Desarrollar MVPs interactivos para presentar los resultados del análisis en Streamlit, Power BI y en una plataforma web. Para mayor comprención del modelo de datos Documentacion Tecnologías Utilizadas y Recursos Utilizados | Tecnologías Utilizadas | Recursos Min. Utilizados | |---------------------------|-----------------------------------------------| | Python 3.10.11 | Sistema Operativo: Windows | | Pandas | Memoria RAM: 16 GB -32 GB...",README.md,2024-05-08 18:56:50,https://github.com/programmingwithclaudio/MVP_ML_TransporteNY,"{""proposito_principal"": ""MLOPS de Predicción de Precios en el Servicio de Taxi en la Región de New York"", ""dominio_aplicacion"": ""Transporte"", ""tipo_proyecto"": [""MLOps"", ""Data Science"", ""Dashboard"", ""Análisis de Datos""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""Streamlit""], ""bases_datos"": [], ""ml_ia"": [""Machine Learning""], ""devops_cloud"": [""MLOps""], ""funcionalidades_clave"": [""Predicción de precios de taxis"", ""Análisis exploratorio de datos"", ""Modelos de Machine Learning para predicción de demanda"", ""Dashboard interactivo""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Transporte de pasajeros"", ""Vehículos eléctricos"", ""Análisis de rentabilidad"", ""Externalidades ambientales""]}"
programmingwithclaudio/n8n-automation-eauren,Sí,Sí,"Proyecto n8n TEST (https://github.com/morhetz/gruvbox) (https://github.com/programmingwithclaudio/dotfiles) (https://opensource.org/licenses/MIT) http://localhost:9001/login Login minioadmin - minioadmin http://localhost:5678/setup Creala por defecto http://localhost:8080/ http://localhost:8080/manager Login http://localhost:8080 - 6f452646de12e76ae1625de209d77862 Utiliza el nombre del servicio en la red interna: Dado que ambos contenedores están en la misma red (), debes emplear el nombre del servicio definido en el docker-compose. En este caso, en lugar de , utiliza: Esto le indica a Evolution API que debe enviar los eventos al contenedor n8n. Verifica la configuración de los parámetros de webhook: Observa que en la configuración actual tienes: Si la intención es que Evolution API escuche o envíe eventos de forma global, es posible que necesites establecerlo en o revisar la documentación de Evolution API para confirmar si se requiere algún otro parámetro para gestionar las llamadas a webhook. Asegúrate de que la URL del servidor esté correctamente configurada: Actualmente, el parámetro en Evolution API está configurado como . Si esta URL se utiliza internamente para construir enlaces o redirigir llamadas, es posible que debas ajustarla para reflejar la red interna o la dirección externa correcta según el flujo de trabajo que estés implementando. Proyecto n8n TEST (https://postimg.cc/hQSpqXzw) Flujo de nodos e intrucciones.",README.md,2025-05-12 20:46:38,https://github.com/programmingwithclaudio/n8n-automation-eauren,"{""proposito_principal"": ""Proyecto de automatización con n8n para pruebas y configuración de flujos de trabajo"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""Automation"", ""Integration Platform""], ""tecnologias_backend"": [""n8n""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""docker-compose""], ""funcionalidades_clave"": [""Webhooks"", ""Integración de servicios"", ""Flujos de automatización"", ""Configuración de contenedores""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Testing"", ""Configuration"", ""Open Source""]}"
programmingwithclaudio/n8n-automation-sells,Sí,Sí,"Proyecto n8n TEST (https://github.com/morhetz/gruvbox) (https://github.com/programmingwithclaudio/dotfiles) (https://opensource.org/licenses/MIT) http://localhost:9001/login Login minioadmin - minioadmin http://localhost:5678/setup Crear http://localhost:8080/ key 6f452646de12e76ae1625de209d77862 http://localhost:5050/ Login admin@local.com - admin http://localhost:8088/login/ Login admin - admin http://localhost:8080/manager Login http://localhost:8080 - 6f452646de12e76ae1625de209d77862 Utiliza el nombre del servicio en la red interna: Dado que ambos contenedores están en la misma red (), debes emplear el nombre del servicio definido en el docker-compose. En este caso, en lugar de , utiliza: Esto le indica a Evolution API que debe enviar los eventos al contenedor n8n. Verifica la configuración de los parámetros de webhook: Observa que en la configuración actual tienes: Si la intención es que Evolution API escuche o envíe eventos de forma global, es posible que necesites establecerlo en o revisar la documentación de Evolution API para confirmar si se requiere algún otro parámetro para gestionar las llamadas a webhook. Asegúrate de que la URL del servidor esté correctamente configurada: Actualmente, el parámetro en Evolution API está configurado como . Si esta URL se utiliza internamente para construir enlaces o redirigir llamadas, es posible que debas ajustarla para reflejar la red interna o la dirección externa correcta según el flujo de trabajo que estés implementando. Proyecto n8n TEST (https://postimg.cc/hQSpqXzw) Flujo de nodos e intrucciones. Reconstruir el servicio eliminando la caché: Endpoints disponibles: Ejemplo de uso: credentials/telegram ngrok http 5678 docker compose down && docker compose up -d docker-compose down && docker-compose up -d docker exec -it evolutionapiv2 curl -X POST http://n8n:5678/webhook -d 'test' docker exec -it n8n-automation-mtc-evolutionapiv2-1 curl -X POST http://n8n:5678/webhook -d 'test' docker exec -it n8n-automation-mtc-evolutionapiv2-1 ping -c 4 n8n docker exec -it n8n-automation-mtc-evolutionapiv2-1 sh -c ""nc -zv n8n 5678"" superset Reconstruir la imagen y ejecutar Después de realizar el cambio en el Dockerfile, reconstruye la imagen: GITSSHCOMMAND=""ssh -i /home/oakdev/yes"" git push --set-upstream origin main docker exec -it n8n sh https://base64.guru/converter/decode/file https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-spanish (https://postimg.cc/BX25ZSPM) docker compose build embedding-service...",README.md,2025-04-09 01:02:49,https://github.com/programmingwithclaudio/n8n-automation-sells,"{""proposito_principal"": ""Automatización de procesos de ventas y comunicación usando n8n y Evolution API"", ""dominio_aplicacion"": ""Automatización de ventas y comunicación empresarial"", ""tipo_proyecto"": [""Workflow Automation"", ""Integration Platform"", ""Bot Platform""], ""tecnologias_backend"": [""n8n"", ""Evolution API""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose"", ""ngrok""], ""funcionalidades_clave"": [""Webhooks"", ""Automatización de flujos de trabajo"", ""Integración con APIs externas"", ""Comunicación por Telegram""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""MIT License"", ""Containerizado""]}"
programmingwithclaudio/n8n-automation-stack,No,Sí,"Proyecto n8n TEST (https://github.com/morhetz/gruvbox) (https://github.com/programmingwithclaudio/dotfiles) (https://opensource.org/licenses/MIT) http://localhost:9001/login Login minioadmin - minioadmin http://localhost:5678/setup Creala por defecto http://localhost:8080/ http://localhost:8080/manager Login http://localhost:8080 - 6f452646de12e76ae1625de209d77862 Utiliza el nombre del servicio en la red interna: Dado que ambos contenedores están en la misma red (), debes emplear el nombre del servicio definido en el docker-compose. En este caso, en lugar de , utiliza: Esto le indica a Evolution API que debe enviar los eventos al contenedor n8n. Verifica la configuración de los parámetros de webhook: Observa que en la configuración actual tienes: Si la intención es que Evolution API escuche o envíe eventos de forma global, es posible que necesites establecerlo en o revisar la documentación de Evolution API para confirmar si se requiere algún otro parámetro para gestionar las llamadas a webhook. Asegúrate de que la URL del servidor esté correctamente configurada: Actualmente, el parámetro en Evolution API está configurado como . Si esta URL se utiliza internamente para construir enlaces o redirigir llamadas, es posible que debas ajustarla para reflejar la red interna o la dirección externa correcta según el flujo de trabajo que estés implementando. Proyecto n8n TEST (https://postimg.cc/hQSpqXzw) Flujo de nodos e intrucciones.",README.md,2025-05-07 22:42:02,https://github.com/programmingwithclaudio/n8n-automation-stack,"{""proposito_principal"": ""Stack de automatización basado en n8n para orquestar flujos de trabajo entre múltiples servicios"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""Automation Stack"", ""Integration Platform""], ""tecnologias_backend"": [""n8n"", ""Evolution API""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose""], ""funcionalidades_clave"": [""Webhooks"", ""Integración de servicios"", ""Orquestación de flujos de trabajo"", ""Gestión de contenedores""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Open Source"", ""Integration Platform""]}"
programmingwithclaudio/Navicat_Patch_v17,No,Sí,"Navicat Premium Cracker (v17) Overview Navicat Premium is a comprehensive database management tool that supports multiple database types such as MySQL, PostgreSQL, SQLite, and more. It offers a user-friendly interface, advanced tools for data modeling, seamless data migration, and an efficient query editor, making it a favorite among developers and database administrators. Benefits of Navicat Premium: Cross-Database Management: Manage multiple databases from a single platform. Data Transfer and Synchronization: Seamlessly migrate data between databases. Visual Query Builder: Create complex queries without writing SQL code. Data Visualization: Generate detailed reports and charts for data analysis. Advanced Security Features: Protect sensitive data with robust encryption. This guide provides simple and clear instructions to enable Navicat Premium v17 using a custom cracker. With just a few steps, you can unlock the full features of Navicat without limitations. This guide provides simple and clear instructions to enable Navicat Premium v17 using a custom cracker. With just a few steps, you can unlock the full features of Navicat without limitations. Key Details: Compatibility: Navicat v17.1.12 (32-bit and 64-bit systems). Files Needed: A patched winmm.dll file that replaces the original. Purpose: Quickly set up Navicat with all its features active. Prerequisites Download Navicat v17: Get version 17 of Navicat from its official website Download the resouce: Obtain the required files from download resouce. Usage Instructions Close Navicat Ensure that Navicat is completely closed. If the program is running, close it before proceeding. Prepare the DLL File Extract the downloaded file: Unzip the cracker files to an accessible location. Depending on your operating system architecture, select the appropriate file: 32-bit system: Use the winmm.dll file from x32Patch.zip. 64-bit system: Use the winmm.dll file from x64Patch.zip. Replace the DLL File Copy the selected winmm.dll file and paste it into the Navicat installation directory. The default location is usually: C:\Program Files\PremiumSoft\Navicat (64-bit) C:\Program Files (x86)\PremiumSoft\Navicat (32-bit) Start Navicat Launch Navicat as usual. If all steps were followed correctly, the cracker should work. Important Notes Version Compatibility: This cracker has been verified for Navicat version 17.1.12. Ensure that your version is compatible. Latest Verified Version：V17.1.12 Select the Correct File: Verify...",README.md,2025-10-11 19:14:16,https://github.com/programmingwithclaudio/Navicat_Patch_v17,"{""proposito_principal"": ""Cracker para activar Navicat Premium v17 sin licencia mediante parche de archivos DLL"", ""dominio_aplicacion"": ""Software de gestión de bases de datos"", ""tipo_proyecto"": [""Cracker"", ""Herramienta de activación"", ""Parche de software""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Parche de archivos DLL"", ""Activación de software sin licencia"", ""Compatibilidad con sistemas 32-bit y 64-bit"", ""Reemplazo de winmm.dll""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Software pirata"", ""Activación ilegal"", ""Cracker"", ""Parche"", ""Navicat""]}"
programmingwithclaudio/nest-angular-openai,Sí,Sí,BACKEND desactivate add SECTION 3 VIDEO 5 Enlaces mermaid FRONTED Angular ingles,README.md,2025-02-09 02:22:12,https://github.com/programmingwithclaudio/nest-angular-openai,"{""proposito_principal"": ""Proyecto educativo que integra NestJS backend con Angular frontend para demostrar integración con OpenAI API"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Full Stack Web"", ""Educational Template""], ""tecnologias_backend"": [""NestJS""], ""tecnologias_frontend"": [""Angular""], ""bases_datos"": [], ""ml_ia"": [""OpenAI API""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Integración con OpenAI"", ""Generación de diagramas Mermaid""], ""lenguajes_programacion"": [""TypeScript"", ""JavaScript""], ""tags_adicionales"": [""Educational"", ""Template"", ""Integration Demo""]}"
programmingwithclaudio/next-sidebar-basic,Sí,Sí,"This is a Next.js project bootstrapped with (https://nextjs.org/docs/app/api-reference/cli/create-next-app). Getting Started First, run the development server: Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying . The page auto-updates as you edit the file. This project uses (https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load Geist, a new font family for Vercel. Learn More To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.",README.md,2025-01-17 23:52:38,https://github.com/programmingwithclaudio/next-sidebar-basic,"{""proposito_principal"": ""Proyecto básico de Next.js con sidebar como plantilla de inicio"", ""dominio_aplicacion"": ""Desarrollo Web"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Vercel""], ""funcionalidades_clave"": [""Optimización automática de fuentes"", ""Hot reload en desarrollo"", ""Deploy automático en Vercel""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Template"", ""Open Source"", ""Bootstrapped""]}"
programmingwithclaudio/nextjs-tailwindcss-blog,Sí,Sí,"Next.js 13 Blog Personal Blog-ProgrammingWithClaudio Screenshot (https://postimg.cc/KkR3trNF) Resources Used in This Project Thank you CodeBucks Character image in the About page created by using Bing Search(https://www.bing.com/). Lottie animation in the contact page: from here Fonts from https://fonts.google.com/ Icons from https://iconify.design/ All the images used in the blogs: Photo by LiminalMessiah on DeviantArt Photo by Carlos Muza on Unsplash Photo by Marvin Meyer on Unsplash Photo by Paul Esch-Laurent on Unsplash Photo by Kelly Sikkema on Unsplash Photo by Lauren Mancke on Unsplash Photo by Luca Bravo on Unsplash Photo by Christina @ wocintechchat.com on Unsplash Photo by C D-X on Unsplash Photo by charlesdeluvio on Unsplash Photo by Emile Perron on Unsplash Photo by Roman Synkevych on Unsplash This is a Next.js project bootstrapped with (https://github.com/vercel/next.js/tree/canary/packages/create-next-app). Getting Started First, run the development server: Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying . The page auto-updates as you edit the file. This project uses (https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font. Learn More To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.",README.md,2024-06-10 23:29:11,https://github.com/programmingwithclaudio/nextjs-tailwindcss-blog,"{""proposito_principal"": ""Blog personal con Next.js 13"", ""dominio_aplicacion"": ""Blogging/Publicación de contenido"", ""tipo_proyecto"": [""Full Stack Web"", ""Blog""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js"", ""Tailwind CSS""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Optimización de fuentes"", ""Despliegue en Vercel"", ""Animaciones Lottie""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Open Source"", ""Template"", ""Personal Blog""]}"
programmingwithclaudio/nextjs_mongodb_app,Sí,Sí,"This is a Next.js project bootstrapped with (https://github.com/vercel/next.js/tree/canary/packages/create-next-app). Getting Started First, run the development server: Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying . The page auto-updates as you edit the file. This project uses (https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font. Learn More To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.",README.md,2024-05-08 20:02:36,https://github.com/programmingwithclaudio/nextjs_mongodb_app,"{""proposito_principal"": ""Proyecto base de Next.js con configuración inicial para desarrollo web"", ""dominio_aplicacion"": ""Desarrollo Web"", ""tipo_proyecto"": [""Full Stack Web""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Vercel""], ""funcionalidades_clave"": [""Optimización de fuentes"", ""Hot reloading en desarrollo"", ""Deploy automático en Vercel""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Template"", ""Starter Project"", ""Open Source""]}"
programmingwithclaudio/nexts-dashboard-dark,Sí,Sí,"This is a Next.js project bootstrapped with (https://nextjs.org/docs/app/api-reference/cli/create-next-app). Getting Started First, run the development server: Open http://localhost:3000 with your browser to see the result. You can start editing the page by modifying . The page auto-updates as you edit the file. This project uses (https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load Geist, a new font family for Vercel. Learn More To learn more about Next.js, take a look at the following resources: Next.js Documentation - learn about Next.js features and API. Learn Next.js - an interactive Next.js tutorial. You can check out the Next.js GitHub repository - your feedback and contributions are welcome Deploy on Vercel The easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js. Check out our Next.js deployment documentation for more details.",README.md,2025-01-23 01:46:01,https://github.com/programmingwithclaudio/nexts-dashboard-dark,"{""proposito_principal"": ""Dashboard con tema oscuro construido con Next.js"", ""dominio_aplicacion"": ""Desarrollo Web"", ""tipo_proyecto"": [""Full Stack Web"", ""Dashboard""], ""tecnologias_backend"": [""Next.js""], ""tecnologias_frontend"": [""Next.js""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Vercel""], ""funcionalidades_clave"": [""Optimización automática de fuentes"", ""Hot reload en desarrollo"", ""Deploy automático en Vercel""], ""lenguajes_programacion"": [""JavaScript"", ""TypeScript""], ""tags_adicionales"": [""Template"", ""Open Source""]}"
programmingwithclaudio/notesdev-skill-dev,Sí,No,Sin documentación disponible,,2024-12-14 21:00:45,https://github.com/programmingwithclaudio/notesdev-skill-dev,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/phones-etl-mssql,Sí,Sí,"Proyecto v0.0.2: ETL Automatización de números móviles Reniec Para instalar las librerías necesarias para ejecutar el código que mencionas, puedes utilizar , que es el instalador de paquetes de Python. A continuación, te detallo las librerías y el comando para instalarlas: Registros con python (https://postimg.cc/JDmkd5Tm) Registros comprobacion -part1 (https://postimg.cc/87KCKTSQ) Registros comprobacion -part2 (https://postimg.cc/QVfT05cG) Verificación de números móviles (https://postimg.cc/nCMQ5z1s) Obtencion registros Obtencion de AUTOMATIZACION REVISAR LOS CAMBIOS",README.md,2024-10-28 17:17:16,https://github.com/programmingwithclaudio/phones-etl-mssql,"{""proposito_principal"": ""ETL para automatización de números móviles de Reniec"", ""dominio_aplicacion"": ""Gobierno/Registro Civil"", ""tipo_proyecto"": [""ETL"", ""Automatización""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Extracción de datos de números móviles"", ""Transformación de datos"", ""Carga de datos"", ""Verificación de números móviles"", ""Automatización de procesos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Reniec"", ""Números móviles"", ""Automatización""]}"
programmingwithclaudio/PI_MLOPS_Steam,No,Sí,"Proyecto de Análisis de Datos y Sistema Recomendaciones para Juegos de Steam Descripción del Proyecto Este proyecto utiliza FastAPI y Pydantic para crear una API que ofrece análisis y recomendaciones relacionadas con juegos de Steam. El proyecto se enfoca en el procesamiento y análisis de datos, abordando áreas como análisis de sentimientos en reseñas, estadísticas de usuarios, información de desarrolladores y más. Accede a Deploy del Proyecto: Web API Sustentación de Proyecto: Vídeo YouTube Estructura del Proyecto Main Script (): Contiene la lógica principal de la API utilizando el framework FastAPI. Define varios modelos Pydantic para la validación de datos de entrada y salida. Implementa varios endpoints para acceder a diferentes funcionalidades del proyecto. Procesamiento de Sentimientos (): Clase encargada de realizar el análisis de sentimientos en las reseñas de los juegos. Procesa el DataFrame post ETL y genera una nueva versión con información de sentimientos. Modelado de Algoritmo (): Clase encargada de generar la matriz de carasteristicas para las las reseñas de los juegos. Recibe un dataframe escalado (Post Métodos Label encoder y Hot Enconder) lo que ayuda al redimiento para las estimaciones en el sistema de recomendación. Notebook de Data Science (): Contiene el desarrollo desde la etapa inicial EDA, ETL(Modelamiento), Entrenamiento y adicionalmente al marco de presentación para Henry, propone predecir ¿Si un usuario recomendara o No el juego? combinando y , en proceso para realizar estimaciones en el proyecto. Endpoints Información del Desarrollador: Endpoint: Descripción: Obtiene información detallada sobre un desarrollador específico, incluyendo la cantidad de items y el contenido gratuito en diferentes años. Estadísticas de Usuario: Endpoint: Descripción: Proporciona estadísticas para un usuario específico basadas en su ID de Steam, incluyendo el dinero gastado, el porcentaje de recomendación y la cantidad total de items. Horas Jugadas por Género: Endpoint: Descripción: Muestra las horas jugadas por usuarios para un género específico y el usuario con más horas jugadas en ese género. Desarrolladores Principales por Año: Endpoint: Descripción: Identifica a los desarrolladores principales en un año específico según la cantidad de juegos recomendados. Análisis de Reseñas por Desarrollador: Endpoint: Descripción: Analiza las reseñas de un desarrollador específico, contando la cantidad de reseñas positivas, negativas y neutrales. Sistema de...",README.md,2024-05-08 23:03:47,https://github.com/programmingwithclaudio/PI_MLOPS_Steam,"{""proposito_principal"": ""API para análisis de datos y sistema de recomendaciones de juegos de Steam"", ""dominio_aplicacion"": ""Videojuegos / Plataformas digitales"", ""tipo_proyecto"": [""API REST"", ""Data Science"", ""Sistema de recomendación""], ""tecnologias_backend"": [""FastAPI""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""Análisis de sentimientos"", ""Sistema de recomendación""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Análisis de sentimientos en reseñas"", ""Estadísticas de usuarios"", ""Información de desarrolladores"", ""Horas jugadas por género"", ""Desarrolladores principales por año"", ""Recomendaciones de juegos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""ETL"", ""EDA"", ""Pydantic"", ""Label Encoder"", ""Hot Encoder""]}"
programmingwithclaudio/practicedev,Sí,No,Sin documentación disponible,,2023-10-22 02:19:59,https://github.com/programmingwithclaudio/practicedev,"{""proposito_principal"": ""No se puede determinar el propósito del proyecto debido a falta de documentación e información disponible"", ""dominio_aplicacion"": ""No se puede determinar el dominio de aplicación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/procesos-etl-sql,Sí,Sí,AUTOMATIZACION A PYTHON PARA LA CONEXION AUTOMATIZACION A PYTHON PARA LA CONEXION,README.md,2024-08-21 19:47:36,https://github.com/programmingwithclaudio/procesos-etl-sql,"{""proposito_principal"": ""Automatización de procesos ETL (Extract, Transform, Load) para conexión a bases de datos SQL"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""ETL Pipeline"", ""CLI Tool""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Conexión automatizada a bases de datos"", ""Procesos ETL""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Automatización"", ""SQL""]}"
programmingwithclaudio/programmingwithclaudio,No,Sí,"¡Hola Me llamo Claudio, desarrollo proyectos de Machine Learning Operations (MLOps) en industrias como Hotelería, Contac Center, Transportes y otros. Actualmente colaboró desarrollando soluciones empresariales en procesos de Base de Datos, ETL, Big Data y aplicaciones Web's con IA en Python. ¿Tienes dudas o sugerencias? ¿Los procesos de Analítica de Negocios experimentan constantemente demoras? ¿Como genera valor la IA y la automatización de procesos en negocios, áreas y /o componentes?, toma la decisión de implementar una aplicación Web desarrollada con la metodología Científica de los Datos. Desarrollo demos y consultorías adaptadas a tus necesidades y objetivos. Para cualquier consulta, pueden contactarme en clblommberg@gmail.com",README.md,2024-07-16 01:01:02,https://github.com/programmingwithclaudio/programmingwithclaudio,"{""proposito_principal"": ""Desarrollo de soluciones empresariales en procesos de Base de Datos, ETL, Big Data y aplicaciones Web's con IA en Python"", ""dominio_aplicacion"": ""Hotelería, Contact Center, Transportes"", ""tipo_proyecto"": [""Aplicaciones Web con IA"", ""Procesos ETL"", ""Soluciones de Big Data""], ""tecnologias_backend"": [""Python""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""Machine Learning Operations (MLOps)"", ""IA""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Automatización de procesos"", ""Analítica de Negocios"", ""Metodología Científica de los Datos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Consultoría"", ""Soluciones Empresariales"", ""Demos""]}"
programmingwithclaudio/Proym6_Integrador_henry,No,Sí,"Machine Learning a Servicios Médicos Indicador de biopsias prostáticas. Preparación de Datos Se estandarizaron 550 registros médicos de pacientes para el análisis. Un 40% de los casos tenían datos faltantes en ciertos campos como mediciones de antígeno prostático. Se categorizaron 12 variables como factores de riesgo relevantes. Revisar supuestos: 1EDA.ipynb Modelado Se entrenó un modelo de Árboles de Decisión para predecir riesgo de biopsias. El modelo tuvo una precisión de 41% y capacidad de detección (recall) de 52% en el conjunto de prueba. Sólo predijo correctamente un 27% de casos de alto riesgo en la validación. Revisar supuestos: 2Modelamiento.ipynb 3Entrenamiento.ipynb 4Prediciones.ipynb Explicación a Usuarios Médicos Con la precisión actual del 41%, más de la mitad de las predicciones serían erróneas, lo cual es inaceptable. Para uso clínico se debería alcanzar al menos una precisión del 80%, con recall sobre 60%. Estamos trabajando en conseguir datos adicionales de pacientes para mejorar la detección de los casos complejos. Agradecemos su paciencia; los mantendremos informados sobre el progreso en las próximas semanas. En resumen, el desempeño actual del 41% de precisión no es suficiente para uso médico confiable. Trabajaremos en mejorar la calidad del modelo mediante la incorporación de más datos de pacientes. Por favor contáctennos ante cualquier duda",README.md,2024-02-25 01:47:33,https://github.com/programmingwithclaudio/Proym6_Integrador_henry,"{""proposito_principal"": ""Modelo de Machine Learning para predecir riesgo de biopsias prostáticas en servicios médicos"", ""dominio_aplicacion"": ""Salud"", ""tipo_proyecto"": [""Machine Learning"", ""Análisis de datos médicos""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""Árboles de Decisión""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Predicción de riesgo de biopsias prostáticas"", ""Análisis de factores de riesgo médicos"", ""Validación de modelos médicos""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Investigación médica"", ""Modelo experimental"", ""Datos médicos""]}"
programmingwithclaudio/pyo_ops_vial,Sí,Sí,"Como especialista en ciencia de datos, manejaría los campos de fecha y hora del siguiente modo: Separar la columna en varias columnas: La columna contiene varios componentes (año, mes, día) combinados en un solo valor de cadena de texto. Para facilitar el análisis, es recomendable separar estos componentes en columnas individuales. Puedo crear tres nuevas columnas: , y . Separar la columna en varias columnas: De manera similar a la columna , la columna contiene la hora y los minutos combinados. Puedo crear dos nuevas columnas: y . Convertir las columnas de fecha y hora a formato de fecha y hora adecuado: Una vez que tengo columnas separadas para año, mes, día, hora y minuto, puedo crear una nueva columna que combine estos componentes en un formato de fecha y hora adecuado. Eliminar las columnas originales y : Después de crear las nuevas columnas separadas y la columna , puedo eliminar las columnas originales y si ya no son necesarias. Aplicar formatos de fecha y hora según sea necesario: Dependiendo de los requisitos del análisis, puedo aplicar formatos específicos a las columnas de fecha y hora utilizando el método de pandas. Estos pasos garantizarán que los campos de fecha y hora estén en un formato adecuado y separado para un análisis más efectivo. Además, tener columnas separadas para año, mes, día, hora y minuto facilitará la realización de operaciones y cálculos relacionados con fechas y horas, como filtrar por rangos de fechas, agrupar por períodos de tiempo, calcular duraciones, etc. LEER GEOESPACIALES EN POSTGRES En sistemas Windows, la instalación de PostGIS puede ser un poco diferente. Aquí hay una guía paso a paso para instalar PostGIS en PostgreSQL en Windows: Descargar PostGIS Binaries: Visita la página de descargas de PostGIS y selecciona la versión que corresponde a tu versión de PostgreSQL y arquitectura (32-bit o 64-bit). Instalar PostGIS Binaries: Ejecuta el instalador que descargaste y sigue las instrucciones del asistente de instalación. Asegurar la ubicación del archivo postgis.control: Después de la instalación, verifica si el archivo está en la carpeta correcta. Puede estar en una ubicación similar a: Asegúrate de que este archivo exista. Agregar la extensión PostGIS desde PostgreSQL: Abre tu consola de PostgreSQL y ejecuta: Si recibes algún error relacionado con la ruta del archivo , puedes proporcionar la ruta completa del archivo. Por ejemplo: Con estos pasos, deberías poder instalar y habilitar la extensión PostGIS en tu base...",README.md,2024-03-09 19:52:55,https://github.com/programmingwithclaudio/pyo_ops_vial,"{""proposito_principal"": ""Procesamiento y análisis de datos de tráfico vial con componentes geoespaciales"", ""dominio_aplicacion"": ""Transporte y movilidad urbana"", ""tipo_proyecto"": [""Data Processing"", ""Data Analysis""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""PostgreSQL"", ""PostGIS""], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Procesamiento de datos de fecha y hora"", ""Análisis geoespacial"", ""Separación de componentes temporales"", ""Conversión de formatos de fecha/hora""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Data Science"", ""Geospatial Analysis"", ""PostGIS Extension"", ""Windows Installation Guide""]}"
programmingwithclaudio/science-weekly-sales-transactions,Sí,Sí,TRAER CAMPOS DE UNA RELACION A OTRA TABLA AGRUPAR CATEGORIAS FILTROS AVANZADOS REVISAR Incio en GCP - Google Cloud Console Nuevo Proyecto nombre del proyecto: test-automatization-drive Apis y Servicios: Biblioteca Google Drive Api: habilitar // Google Sheets API Pantalla de concentimiento: nombre de la aplicacion : test-atomatization-drive-app Correo electrónico de asistencia del usuario: clblommberg@gmail.com Publico : Usuarios Externos Información de contacto: clblommberg@gmail.com Acesso a datos: marcar todos guardar Clientes: Crear un Cliente opciones: aplicacion web android extension de chrome ios tvs y dispositivos de entrada limitada app de escritorio plataforma universal de windows (uwp) Automatizaciones Ve a Credenciales Crear credenciales ID de cliente de OAuth 2.0 Tipo de aplicación: Aplicación de escritorio Nombre: python-automation-client Descarga el archivo JSON de credenciales Guarda ese archivo como credentials.json y úsalo en tu script https://medium.com/data-science/how-to-download-a-specific-sheet-by-name-from-a-google-spreadsheet-as-a-csv-file-e8c7b4b79f39,README.md,2025-04-12 06:33:54,https://github.com/programmingwithclaudio/science-weekly-sales-transactions,"{""proposito_principal"": ""Automatización de descarga de hojas específicas de Google Sheets como archivos CSV"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""CLI Tool"", ""Script de automatización""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Google Cloud Platform"", ""Google Drive API"", ""Google Sheets API""], ""funcionalidades_clave"": [""Autenticación OAuth 2.0"", ""Descarga de hojas específicas de Google Sheets"", ""Conversión a CSV"", ""Gestión de credenciales""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Google APIs"", ""Automatización"", ""Scripting""]}"
programmingwithclaudio/scraping-etl-db,Sí,Sí,ETL y Scraping pipeline pyspark-example ml-pipeline e2e-data-engineering 23 patrones de diseño - erich gamma .pdf,README.md,2025-03-16 01:17:26,https://github.com/programmingwithclaudio/scraping-etl-db,"{""proposito_principal"": ""Pipeline de ETL y scraping para procesamiento de datos con PySpark"", ""dominio_aplicacion"": ""Data Engineering"", ""tipo_proyecto"": [""Pipeline de datos"", ""ETL"", ""Scraper""], ""tecnologias_backend"": [""PySpark""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Extracción de datos (scraping)"", ""Transformación de datos"", ""Carga de datos (ETL)"", ""Procesamiento distribuido con Spark""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Patrones de diseño"", ""Data Engineering"", ""Ejemplo educativo""]}"
programmingwithclaudio/sem2_ia_multimodal,No,Sí,"🧠 Proyecto: Agente Multimodal con Redis y Gradio 📋 Requisitos del Entorno | Componente | Versión | Descripción | | ----------- | ---------------------- | ---------------------------------------------------------- | | Windows | 10 | Sistema operativo base. | | Python | 3.12.10 | Lenguaje principal para scripts ETL y embeddings. | | Redis | 3.x | Almacenamiento clave-valor para cache y consultas rápidas. | | FFmpeg | 8.x (Essentials Build) | Requerido para el procesamiento de audio. | | FAISS | Compatible con CPU | Motor de búsqueda vectorial optimizado. | ⚙️ Instalación de Dependencias 1️⃣ Instalar FFmpeg Opción A – Desde WinGet (recomendada): Opción B – Manual: Descarga desde https://ffmpeg.org/download.html Extrae el contenido en una ruta segura, por ejemplo: Verificar instalación: Ubicación del ejecutable (PowerShell): Configurar la ruta en el script principal: 🧩 Estructura del Proyecto 🚀 Flujo de Ejecución Antes de ejecutar, asegúrate de establecer el directorio base del proyecto: Paso 1: Generar Embeddings Ejecuta el script: Este proceso: Lee el CSV de entrada (≈ 38 millones de registros) Divide el dataset en chunks de 500 registros Genera embeddings y los almacena localmente Paso 2: Levantar el Agente Ejecuta el script: El agente: Conecta a Redis 3 Carga los embeddings en FAISS Expone una interfaz Gradio para interacción en tiempo real (por ejemplo, búsqueda semántica, consultas naturales o audio-inputs) 🔁 Flujo General del Sistema (https://postimg.cc/YL1thT8X) 🧮 Recomendaciones de Rendimiento Utiliza FAISS con índices HNSW o IVFFlat para acelerar búsquedas vectoriales. Aumenta el de Redis si el dataset excede 4 GB. Ejecuta el proceso en modo batch durante las horas de baja carga. Considera un SSD para almacenar los embeddings (mayor IOPS). 🧰 Comandos Útiles | Tarea | Comando | | ---------------------------- | --------------------------------- | | Verificar Redis activo | | | Monitorear memoria Redis | | | Verificar instalación Python | | | Instalar dependencias Python | |",README.md,2025-11-12 04:53:22,https://github.com/programmingwithclaudio/sem2_ia_multimodal,"{""proposito_principal"": ""Agente multimodal para búsqueda semántica y consultas naturales con procesamiento de audio"", ""dominio_aplicacion"": ""Inteligencia Artificial"", ""tipo_proyecto"": [""Agente IA"", ""Sistema de búsqueda semántica"", ""Interfaz multimodal""], ""tecnologias_backend"": [""Python""], ""tecnologias_frontend"": [""Gradio""], ""bases_datos"": [""Redis""], ""ml_ia"": [""FAISS"", ""Embeddings"", ""Búsqueda vectorial"", ""Procesamiento de audio""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Generación de embeddings"", ""Búsqueda semántica"", ""Procesamiento de audio"", ""Interfaz en tiempo real"", ""Cache con Redis"", ""Consultas naturales""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""ETL"", ""Multimodal"", ""Vector database"", ""CPU optimized""]}"
programmingwithclaudio/sem3_ia_simulator_market,No,Sí,"🧠 MONETO — Simulador de Mercado Contextual Basado en IA Descripción General del Proyecto Moneto no es solo un generador de opiniones: es un simulador de mercado contextual, diseñado para ayudar a empresas peruanas y latinoamericanas a predecir la recepción de productos antes de su lanzamiento. Su objetivo es reducir el riesgo comercial y maximizar las oportunidades de éxito mediante opiniones simuladas realistas, derivadas de modelos de lenguaje e inteligencia contextual adaptados al entorno local. 🎯 Propósito Permitir a las empresas: Evaluar la aceptación potencial de nuevos productos. Simular escenarios de percepción del mercado. Obtener insights accionables antes de invertir en campañas reales. Arquitectura Funcional Flujo de Trabajo y Etapas Clave 1️⃣ Definición del Producto Explicación y sustentación: Definición del producto: Se comienza con el cliente definiendo el producto. Es crucial saber si es nuevo o existente para ajustar la generación de opiniones. Para productos nuevos, se perfilan mercados objetivo; para existentes, se analiza la competencia. Campos clave: Se recopila información básica del producto (nombre, empresa, detalles) que servirá para generar opiniones realistas. Almacenamiento en Redis: Se usa Redis para guardar la sesión y mantener el estado del proceso. 2️⃣ Configuración Analítica Redis almacena el contexto y mantiene la sesión activa. Se define una distribución objetivo de sentimientos (por ejemplo: 55% positivo, 30% neutro, 15% negativo). 3️⃣ Selección del Motor Matemático | Motor | Descripción | Uso Ideal | | --------------------- | ------------------------------------------------------------------------- | --------------------- | | 🎯 SMART-MATCH | Distribución basada en correlación semántica entre mercado y sentimiento. | Estudios predictivos. | | ⚡ DYNAMIC-BALANCE | Ajuste iterativo según desviación de tendencia. | Monitoreo continuo. | | 🎲 REAL-VADER | Feedback loop con analizador VADER + contexto local. | Validación final. | 4️⃣ Generación Contextual (DeepSeek) El modelo DeepSeek produce opiniones en lenguaje natural considerando: Modismos locales (ej. “ta bueno”, “pasadito”, “pa’ probar”) Comportamientos de consumo realistas Distribución estadística definida Ejemplo: 5️⃣ Validación Análisis de sentimiento con VADER. Comparación entre la distribución esperada y la real obtenida. Si la diferencia Δ 2%, el sistema recalibra los parámetros y regenera datos. 6️⃣ Salidas del Sistema Archivos: CSV / JSON Visualización:...",README.md,2025-11-04 18:23:31,https://github.com/programmingwithclaudio/sem3_ia_simulator_market,"{""proposito_principal"": ""Simulador de mercado contextual basado en IA para predecir la recepción de productos antes de su lanzamiento"", ""dominio_aplicacion"": ""Análisis de mercado y comercio"", ""tipo_proyecto"": [""Simulador"", ""Herramienta de análisis predictivo"", ""Sistema de generación de opiniones""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""Redis""], ""ml_ia"": [""DeepSeek"", ""VADER"", ""Modelos de lenguaje"", ""Análisis de sentimiento"", ""Generación de opiniones contextuales""], ""devops_cloud"": [], ""funcionalidades_clave"": [""Generación de opiniones simuladas realistas"", ""Análisis de sentimiento con VADER"", ""Distribución estadística de sentimientos"", ""Validación y recalibración automática"", ""Almacenamiento de sesión en Redis"", ""Múltiples motores matemáticos (SMART-MATCH, DYNAMIC-BALANCE, REAL-VADER)"", ""Exportación de datos en CSV/JSON"", ""Visualización de resultados""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Contexto peruano y latinoamericano"", ""Simulación de mercado"", ""Predicción comercial"", ""Reducción de riesgo""]}"
programmingwithclaudio/sem4_ia_agente_documentador,No,Sí,"Asistente Inteligente de Documentación para Objetivo: asistir en infromación sobre detalles ténicos de la demo y generar una documentación . Usos en frameworks de python (Directo) Configurar entorno , api llm's en y Ejecutar directamente este comando en el directorio principal. (Opcional) Clonar el proyecto y directamente saltar a . 1 Deploy mediante Docker (Kernel-Linux 5.15) (modo contenedor) 1.1 renombrar el por 1.2 Ejecución mediante Docker (modo contenedor) Validar acceso en: Ruta: http://localhost:8000/docs 2 Deploy Agente local Ruta: http://localhost:7865",README.md,2025-11-13 01:28:59,https://github.com/programmingwithclaudio/sem4_ia_agente_documentador,"{""proposito_principal"": ""Asistente Inteligente de Documentación para generar documentación técnica de proyectos"", ""dominio_aplicacion"": ""Desarrollo de Software"", ""tipo_proyecto"": [""API REST"", ""Agente de IA""], ""tecnologias_backend"": [""FastAPI""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [""API LLM's""], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Generación de documentación automática"", ""Asistencia técnica para detalles de demo"", ""API REST con documentación interactiva""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Documentación Automática"", ""Agente Inteligente""]}"
programmingwithclaudio/service-etl-wst,Sí,Sí,"ETL y Ploting Etl Modelo class call-center-anonymous-bank Churn-Prediction Callcenterdataanalist practicaadvancedsql streamlit run app.py Entiendo. Basándome en la configuración de PostgreSQL que me has mostrado y en los comandos docker que proporcionaste para MariaDB y phpMyAdmin, voy a crear un archivo equivalente para MariaDB y phpMyAdmin. Aquí tienes: Este archivo YAML configura dos servicios: : Este es el servicio de MariaDB. Usa la imagen . Expone el puerto de MariaDB (3306) al host. Usa un volumen para persistir los datos. Configura las variables de entorno para el usuario, contraseña y nombre de la base de datos. : Este es el servicio de phpMyAdmin para administrar la base de datos. Depende del servicio . Usa la imagen . Expone el puerto 80 del contenedor al host. Configura las variables de entorno para conectarse a MariaDB. Para usar este archivo, necesitarás definir las siguientes variables de entorno en un archivo en el mismo directorio: Asegúrate de cambiar estos valores según tus necesidades. También, nota que he definido el volumen como externo, al igual que en tu ejemplo de PostgreSQL. Si prefieres crear el volumen automáticamente, puedes cambiar esa parte a: ¿Quieres que explique algo más sobre esta configuración? USE ; SELECT FROM .calls; SELECT FROM calls; Para abordar las preguntas analíticas planteadas y desarrollar funciones en Python que nos permitan analizar los datos según las relaciones entre , , y , vamos a proceder a implementar cada función específica. A continuación, te mostraré cómo podemos estructurar estas funciones: Funciones para Analizar los Datos Para df1 (Datos de Contratación y Servicio al Cliente): Volumen de llamadas por tipo de usuario: Analizaremos la distribución de llamadas entre diferentes tipos de usuarios. Distribución de estados de los casos de servicio al cliente: Investigaremos cuántos casos están en diferentes estados. Motivos de rechazo más comunes: Identificaremos los principales motivos por los cuales se rechazan los casos de servicio. Patrones en la hora de inicio y fin del servicio al cliente: Determinaremos si hay horas del día específicas en las que se concentra más el inicio o fin de las llamadas de servicio. Productos más solicitados o con más casos de servicio: Analizaremos la frecuencia con la que se reportan casos relacionados con diferentes productos. Para df2 (Datos de Agentes y Actividad de Login): Frecuencia de actividad de login por agente: Evaluar la cantidad de veces que cada agente...",README.md,2024-08-19 15:47:43,https://github.com/programmingwithclaudio/service-etl-wst,"{""proposito_principal"": ""ETL y análisis de datos para predicción de churn en call center bancario"", ""dominio_aplicacion"": ""Banca"", ""tipo_proyecto"": [""ETL"", ""Dashboard"", ""Data Analysis""], ""tecnologias_backend"": [""Python"", ""Streamlit""], ""tecnologias_frontend"": [""Streamlit""], ""bases_datos"": [""PostgreSQL"", ""MariaDB""], ""ml_ia"": [""Churn-Prediction""], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Procesamiento ETL"", ""Análisis de datos de call center"", ""Visualización de datos"", ""Predicción de churn"", ""Consultas SQL avanzadas""], ""lenguajes_programacion"": [""Python"", ""SQL""], ""tags_adicionales"": [""Call Center Analytics"", ""Advanced SQL"", ""Data Pipeline""]}"
programmingwithclaudio/services-etl-db,Sí,Sí,"ETL y Ploting Etl Modelo class call-center-anonymous-bank Churn-Prediction Callcenterdataanalist practicaadvancedsql streamlit run app.py Entiendo. Basándome en la configuración de PostgreSQL que me has mostrado y en los comandos docker que proporcionaste para MariaDB y phpMyAdmin, voy a crear un archivo equivalente para MariaDB y phpMyAdmin. Aquí tienes: Este archivo YAML configura dos servicios: : Este es el servicio de MariaDB. Usa la imagen . Expone el puerto de MariaDB (3306) al host. Usa un volumen para persistir los datos. Configura las variables de entorno para el usuario, contraseña y nombre de la base de datos. : Este es el servicio de phpMyAdmin para administrar la base de datos. Depende del servicio . Usa la imagen . Expone el puerto 80 del contenedor al host. Configura las variables de entorno para conectarse a MariaDB. Para usar este archivo, necesitarás definir las siguientes variables de entorno en un archivo en el mismo directorio: Asegúrate de cambiar estos valores según tus necesidades. También, nota que he definido el volumen como externo, al igual que en tu ejemplo de PostgreSQL. Si prefieres crear el volumen automáticamente, puedes cambiar esa parte a: ¿Quieres que explique algo más sobre esta configuración? USE ; SELECT FROM .calls; SELECT FROM calls; Para abordar las preguntas analíticas planteadas y desarrollar funciones en Python que nos permitan analizar los datos según las relaciones entre , , y , vamos a proceder a implementar cada función específica. A continuación, te mostraré cómo podemos estructurar estas funciones: Funciones para Analizar los Datos Para df1 (Datos de Contratación y Servicio al Cliente): Volumen de llamadas por tipo de usuario: Analizaremos la distribución de llamadas entre diferentes tipos de usuarios. Distribución de estados de los casos de servicio al cliente: Investigaremos cuántos casos están en diferentes estados. Motivos de rechazo más comunes: Identificaremos los principales motivos por los cuales se rechazan los casos de servicio. Patrones en la hora de inicio y fin del servicio al cliente: Determinaremos si hay horas del día específicas en las que se concentra más el inicio o fin de las llamadas de servicio. Productos más solicitados o con más casos de servicio: Analizaremos la frecuencia con la que se reportan casos relacionados con diferentes productos. Para df2 (Datos de Agentes y Actividad de Login): Frecuencia de actividad de login por agente: Evaluar la cantidad de veces que cada agente...",README.md,2024-07-21 06:59:28,https://github.com/programmingwithclaudio/services-etl-db,"{""proposito_principal"": ""ETL y análisis de datos para predicción de churn en call center bancario"", ""dominio_aplicacion"": ""Banca/Finanzas"", ""tipo_proyecto"": [""ETL"", ""Dashboard"", ""Data Analysis""], ""tecnologias_backend"": [""Streamlit""], ""tecnologias_frontend"": [""Streamlit""], ""bases_datos"": [""PostgreSQL"", ""MariaDB""], ""ml_ia"": [""Churn Prediction""], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Procesamiento ETL"", ""Análisis de datos de call center"", ""Visualización de datos"", ""Gestión de base de datos""], ""lenguajes_programacion"": [""Python"", ""SQL""], ""tags_adicionales"": [""Call Center Analytics"", ""Advanced SQL"", ""phpMyAdmin""]}"
programmingwithclaudio/spark-etl-postgres-example,Sí,Sí,,README.md,2024-11-04 14:13:14,https://github.com/programmingwithclaudio/spark-etl-postgres-example,"{""proposito_principal"": ""No se puede determinar - falta información sobre el propósito del proyecto"", ""dominio_aplicacion"": ""No se puede determinar - falta información sobre el dominio de aplicación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/spring-boot-login-example,Sí,Sí,"Spring Boot Login example with Spring Security, MySQL and JWT Build a Spring Boot Login and Registration example (Rest API) that supports JWT with HttpOnly Cookie. You’ll know: Appropriate Flow for User Login and Registration with JWT and HttpOnly Cookies Spring Boot Rest Api Architecture with Spring Security How to configure Spring Security to work with JWT How to define Data Models and association for Authentication and Authorization Way to use Spring Data JPA to interact with MySQL Database User Registration, Login and Authorization process. Spring Boot Server Architecture with Spring Security You can have an overview of our Spring Boot Server with the diagram below: For more detail, please visit: Spring Boot Login example with MySQL and JWT For H2 Embedded database For MongoDB Working with Front-end: Angular 12 / Angular 13 / Angular 14 / Angular 15 / Angular 16 / Angular 17 React / React Redux Dependency – If you want to use PostgreSQL: – or MySQL: Configure Spring Datasource, JPA, App properties Open For PostgreSQL: For MySQL Run Spring Boot application Run following SQL insert statements Refresh Token Spring Boot Refresh Token with JWT example More Practice: Spring Boot File upload example with Multipart File Exception handling: @RestControllerAdvice example in Spring Boot Spring Boot Repository Unit Test with @DataJpaTest Spring Boot Rest Controller Unit Test with @WebMvcTest Spring Boot Pagination & Sorting example Validation: Spring Boot Validate Request Body Documentation: Spring Boot and Swagger 3 example Caching: Spring Boot Redis Cache example Associations: JPA/Hibernate One To Many example in Spring Boot JPA/Hibernate Many To Many example in Spring Boot JPA/Hibernate One To One example in Spring Boot Deployment: Deploy Spring Boot App on AWS – Elastic Beanstalk Docker Compose Spring Boot and MySQL example Fullstack CRUD App Vue.js + Spring Boot + H2 Embedded database example Vue.js + Spring Boot + MySQL example Vue.js + Spring Boot + PostgreSQL example Angular 8 + Spring Boot + Embedded database example Angular 8 + Spring Boot + MySQL example Angular 8 + Spring Boot + PostgreSQL example Angular 10 + Spring Boot + MySQL example Angular 10 + Spring Boot + PostgreSQL example Angular 11 + Spring Boot + MySQL example Angular 11 + Spring Boot + PostgreSQL example Angular 12 + Spring Boot + Embedded database example Angular 12 + Spring Boot + MySQL example Angular 12 + Spring Boot + PostgreSQL example Angular 13 + Spring Boot + H2 Embedded...",README.md,2024-10-12 02:41:33,https://github.com/programmingwithclaudio/spring-boot-login-example,"{""proposito_principal"": ""Ejemplo de sistema de Login y Registro con Spring Boot que implementa autenticación JWT con cookies HttpOnly"", ""dominio_aplicacion"": ""Desarrollo de Software"", ""tipo_proyecto"": [""API REST"", ""Template"", ""Ejemplo educativo""], ""tecnologias_backend"": [""Spring Boot"", ""Spring Security"", ""Spring Data JPA""], ""tecnologias_frontend"": [""Angular 12"", ""Angular 13"", ""Angular 14"", ""Angular 15"", ""Angular 16"", ""Angular 17"", ""React"", ""React Redux"", ""Vue.js""], ""bases_datos"": [""MySQL"", ""PostgreSQL"", ""H2 Embedded database"", ""MongoDB""], ""ml_ia"": [], ""devops_cloud"": [""Docker Compose"", ""AWS Elastic Beanstalk""], ""funcionalidades_clave"": [""Autenticación JWT"", ""Cookies HttpOnly"", ""Registro de usuarios"", ""Login de usuarios"", ""Autorización"", ""Refresh Token"", ""Subida de archivos"", ""Manejo de excepciones"", ""Validación de request body"", ""Paginación y ordenamiento"", ""Caché con Redis"", ""Documentación con Swagger 3"", ""Pruebas unitarias"", ""Asociaciones JPA/Hibernate""], ""lenguajes_programacion"": [""Java""], ""tags_adicionales"": [""Open Source"", ""Ejemplo educativo"", ""Template"", ""Spring Framework"", ""JWT Authentication"", ""REST API""]}"
programmingwithclaudio/spring-boot-login-examples,No,Sí,"Spring Boot Login example with Spring Security, MySQL and JWT Build a Spring Boot Login and Registration example (Rest API) that supports JWT with HttpOnly Cookie. You’ll know: Appropriate Flow for User Login and Registration with JWT and HttpOnly Cookies Spring Boot Rest Api Architecture with Spring Security How to configure Spring Security to work with JWT How to define Data Models and association for Authentication and Authorization Way to use Spring Data JPA to interact with MySQL Database User Registration, Login and Authorization process. Spring Boot Server Architecture with Spring Security You can have an overview of our Spring Boot Server with the diagram below: For more detail, please visit: Spring Boot Login example with MySQL and JWT For H2 Embedded database For MongoDB Working with Front-end: Angular 12 / Angular 13 / Angular 14 / Angular 15 / Angular 16 / Angular 17 React / React Redux Dependency – If you want to use PostgreSQL: – or MySQL: Configure Spring Datasource, JPA, App properties Open For PostgreSQL: For MySQL Run Spring Boot application Run following SQL insert statements Refresh Token Spring Boot Refresh Token with JWT example More Practice: Spring Boot File upload example with Multipart File Exception handling: @RestControllerAdvice example in Spring Boot Spring Boot Repository Unit Test with @DataJpaTest Spring Boot Rest Controller Unit Test with @WebMvcTest Spring Boot Pagination & Sorting example Validation: Spring Boot Validate Request Body Documentation: Spring Boot and Swagger 3 example Caching: Spring Boot Redis Cache example Associations: JPA/Hibernate One To Many example in Spring Boot JPA/Hibernate Many To Many example in Spring Boot JPA/Hibernate One To One example in Spring Boot Deployment: Deploy Spring Boot App on AWS – Elastic Beanstalk Docker Compose Spring Boot and MySQL example Fullstack CRUD App Vue.js + Spring Boot + H2 Embedded database example Vue.js + Spring Boot + MySQL example Vue.js + Spring Boot + PostgreSQL example Angular 8 + Spring Boot + Embedded database example Angular 8 + Spring Boot + MySQL example Angular 8 + Spring Boot + PostgreSQL example Angular 10 + Spring Boot + MySQL example Angular 10 + Spring Boot + PostgreSQL example Angular 11 + Spring Boot + MySQL example Angular 11 + Spring Boot + PostgreSQL example Angular 12 + Spring Boot + Embedded database example Angular 12 + Spring Boot + MySQL example Angular 12 + Spring Boot + PostgreSQL example Angular 13 + Spring Boot + H2 Embedded...",README.md,2024-10-20 14:24:37,https://github.com/programmingwithclaudio/spring-boot-login-examples,"{""proposito_principal"": ""Ejemplo de sistema de autenticación y autorización con Spring Boot y Spring Security"", ""dominio_aplicacion"": ""Seguridad y Autenticación"", ""tipo_proyecto"": [""API REST"", ""Template"", ""Educational""], ""tecnologias_backend"": [""Spring Boot"", ""Spring Security"", ""Spring Data JPA""], ""tecnologias_frontend"": [""Angular 12"", ""Angular 13"", ""Angular 14"", ""Angular 15"", ""Angular 16"", ""Angular 17"", ""React"", ""React Redux"", ""Vue.js""], ""bases_datos"": [""MySQL"", ""PostgreSQL"", ""H2 Embedded database"", ""MongoDB""], ""ml_ia"": [], ""devops_cloud"": [""Docker Compose"", ""AWS Elastic Beanstalk""], ""funcionalidades_clave"": [""Autenticación JWT"", ""HttpOnly Cookies"", ""Refresh Token"", ""Registro de usuarios"", ""Autorización"", ""Gestión de sesiones"", ""Validación de datos"", ""Caché con Redis"", ""Subida de archivos"", ""Paginación y ordenamiento"", ""Documentación con Swagger 3"", ""Pruebas unitarias"", ""Manejo de excepciones""], ""lenguajes_programacion"": [""Java""], ""tags_adicionales"": [""Open Source"", ""Educational"", ""Template"", ""Authentication Examples"", ""Spring Security Configuration""]}"
programmingwithclaudio/spring-framework-6-proyecto,No,Sí,Como Crear un Proyecto con Spring Framework 6 Tutorial: https://youtu.be/NarBox1LkYc (https://youtu.be/NarBox1LkYc) https://user-images.githubusercontent.com/11830789/231013254-14e5503e-db22-481a-bb44-e79b787cb550.mp4,README.md,2024-03-13 09:17:31,https://github.com/programmingwithclaudio/spring-framework-6-proyecto,"{""proposito_principal"": ""Tutorial educativo para crear un proyecto con Spring Framework 6"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Tutorial"", ""Template"", ""Proyecto de aprendizaje""], ""tecnologias_backend"": [""Spring Framework 6""], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Configuración de Spring Framework 6"", ""Estructura de proyecto básica""], ""lenguajes_programacion"": [""Java""], ""tags_adicionales"": [""Open Source"", ""Educativo"", ""Template""]}"
programmingwithclaudio/sql-transactions-drive,Sí,Sí,AUTOMATIZACIÓN SQL - REPORTE DE ZONAL Requisitos según detalles Google chrome-version-Version: 135.0.7049.95 chrome-driver Instalar librerías Credenciales de google Procedimiento ACTUALIZACIÓN DIARIA Abrir directorio con VSCODE Ejecutar CAPTURAS: Abrir - Ejecutar la Tarea Referencias querysqlmigrada docker system prune -a- --volumes services.msc netstat -aon | findstr :5432,README.md,2025-07-16 10:27:14,https://github.com/programmingwithclaudio/sql-transactions-drive,"{""proposito_principal"": ""Automatización de reportes SQL para actualización diaria de datos zonales"", ""dominio_aplicacion"": ""Data Processing"", ""tipo_proyecto"": [""Automation Script"", ""Data Pipeline""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Ejecución de consultas SQL"", ""Captura de datos"", ""Actualización diaria automática""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Google Chrome Integration"", ""Chrome Driver"", ""VS Code""]}"
programmingwithclaudio/stackdevelopment,Sí,No,Sin documentación disponible,,2024-01-25 05:35:23,https://github.com/programmingwithclaudio/stackdevelopment,"{""proposito_principal"": ""No se puede determinar - sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar - sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/streamlit-multipage-tx,No,Sí,"Streamlit Multipage App - Pre-buils001v (https://postimg.cc/Wqt79h0P) ¡Dale like y suscríbete ProgrammingWithClaudio YouTube Channel para más contenido relacionado. Overview Este proyecto es una aplicación multipágina desarrollada con Streamlit, que proporciona una interfaz interactiva para visualizar datos. Imágenes de la Aplicación: Photo by BHavishyam Verma on LottieFiles Photo by App Claudio Quispe on postimg.cc Características Interfaz intuitiva y fácil de usar. Múltiples páginas para organizar y presentar diferentes funcionalidades. Despliegue sencillo con Docker. Tecnologías Utilizadas Docker 26.1.1 Streamlit =1.22 Python 3.9 Uso Clona el repositorio: Ejemplo de despliegue con Docker: ¡Explora la aplicación en https://app-multipage-tx-diip.streamlit.app/ Contribución Las contribuciones son bienvenidas. Si tienes ideas para nuevas características, por favor abre un issue para discutirlo o envía un pull request. Licencia Distribuido bajo la licencia MIT. Ver para más información.",README.md,2024-05-09 00:57:32,https://github.com/programmingwithclaudio/streamlit-multipage-tx,"{""proposito_principal"": ""Aplicación multipágina con Streamlit para visualización interactiva de datos"", ""dominio_aplicacion"": ""Data Science"", ""tipo_proyecto"": [""Full Stack Web"", ""Dashboard""], ""tecnologias_backend"": [""Streamlit""], ""tecnologias_frontend"": [""Streamlit""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Interfaz multipágina"", ""Visualización de datos"", ""Despliegue con Docker""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Template"", ""MIT License""]}"
programmingwithclaudio/streamlit_base,Sí,Sí,Docker 578mb super Docker 1.13gb,README.md,2024-05-07 04:24:55,https://github.com/programmingwithclaudio/streamlit_base,"{""proposito_principal"": ""Base template para aplicaciones Streamlit con configuración Docker optimizada"", ""dominio_aplicacion"": ""Desarrollo de aplicaciones web"", ""tipo_proyecto"": [""Template"", ""Base de aplicación""], ""tecnologias_backend"": [""Streamlit""], ""tecnologias_frontend"": [""Streamlit""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""Docker""], ""funcionalidades_clave"": [""Configuración Docker optimizada"", ""Reducción de tamaño de imagen Docker""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Template"", ""Optimización Docker""]}"
programmingwithclaudio/surveybackend,Sí,No,Sin documentación disponible,,2024-09-16 02:45:55,https://github.com/programmingwithclaudio/surveybackend,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [""No se puede determinar sin documentación disponible""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/surveybackend-java-mssql,Sí,Sí,"Getting Started Reference Documentation For further reference, please consider the following sections: Official Apache Maven documentation Spring Boot Maven Plugin Reference Guide Create an OCI image Spring Web Spring Data JDBC Guides The following guides illustrate how to use some features concretely: Building a RESTful Web Service Serving Web Content with Spring MVC Building REST services with Spring Accessing data with MySQL Using Spring Data JDBC Maven Parent overrides Due to Maven's design, elements are inherited from the parent POM to the project POM. While most of the inheritance is fine, it also inherits unwanted elements like and from the parent. To prevent this, the project POM contains empty overrides for these elements. If you manually switch to a different parent and actually want the inheritance, you need to remove those overrides. Configurations '1. Instalar Java.mp4' java jdk 11 '2. Instalar NodeJS.mp4' node 14 '3. Instalar GIT.mp4' ok '4. Instalar VSC.mp4' ok '5. Crear Proyecto de Spring.mp4' Dependencias SpringWeb, lombok, h2base, '6. Crear primer controlador.mp4' '7. Instalar MySQL y crear base de datos.mp4' '8. Conectar Spring con la base de datos.mp4' Crea variables en application.properties Servertimezome list Descarga dependencias faltantes en maven repository Configurations Parece que Maven no está instalado en tu sistema. Puedes instalarlo con el siguiente comando: Una vez instalado Maven, vuelve a intentar el comando: En Ubuntu, puedes instalar y mantener múltiples versiones de JDK (Java Development Kit) al mismo tiempo utilizando y . Aquí tienes los pasos detallados para instalar JDK 11 y JDK 17, y cómo alternar entre ellos. Instalar JDK 11 y JDK 17 a. Instalar JDK 11 Abre una terminal y ejecuta el siguiente comando para instalar JDK 11: b. Instalar JDK 17 Para instalar JDK 17, ejecuta: Verificar las instalaciones Puedes verificar que ambos JDK están instalados ejecutando: Y para ver todas las versiones instaladas: Cambiar entre versiones de JDK Para cambiar la versión de JDK predeterminada, puedes usar : Verás una lista de todas las versiones de Java instaladas. Simplemente selecciona la que quieras usar como predeterminada. Configurar variables de entorno (opcional) Si deseas configurar una versión específica de Java para un proyecto o sesión, puedes hacerlo configurando las variables de entorno y . a. Editar el archivo o Abre el archivo (o si usas Zsh) en tu editor de texto: Añade las siguientes líneas al final del archivo,...",README.md,2024-08-11 17:59:47,https://github.com/programmingwithclaudio/surveybackend-java-mssql,"{""proposito_principal"": ""Backend para sistema de encuestas desarrollado con Spring Boot"", ""dominio_aplicacion"": ""Encuestas y recolección de datos"", ""tipo_proyecto"": [""API REST"", ""Backend Service""], ""tecnologias_backend"": [""Spring Boot"", ""Spring Web"", ""Spring Data JDBC""], ""tecnologias_frontend"": [], ""bases_datos"": [""MySQL"", ""H2 Database""], ""ml_ia"": [], ""devops_cloud"": [""Maven""], ""funcionalidades_clave"": [""Conexión con base de datos MySQL"", ""Configuración de variables en application.properties"", ""Gestión de dependencias Maven""], ""lenguajes_programacion"": [""Java""], ""tags_adicionales"": [""Tutorial/Guía"", ""Configuración de entorno de desarrollo""]}"
programmingwithclaudio/surveyfrontend,Sí,Sí,Surveyfrontend This project was generated with Angular CLI version 17.3.9. Primeros pasos npm instal -g @angular/cli@17.3.9 ng new project-name ng serve (localhost),README.md,2024-10-14 18:01:25,https://github.com/programmingwithclaudio/surveyfrontend,"{""proposito_principal"": ""Aplicación frontend para encuestas desarrollada con Angular"", ""dominio_aplicacion"": ""Encuestas y formularios"", ""tipo_proyecto"": [""Frontend Web"", ""SPA (Single Page Application)""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""Angular"", ""Angular CLI""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Generación de proyectos Angular"", ""Servidor de desarrollo local""], ""lenguajes_programacion"": [""TypeScript""], ""tags_adicionales"": [""Angular CLI Generated"", ""Template Project""]}"
programmingwithclaudio/talenpich_rrhh,Sí,No,Sin documentación disponible,,2024-08-11 14:57:54,https://github.com/programmingwithclaudio/talenpich_rrhh,"{""proposito_principal"": ""No se puede determinar por falta de documentación"", ""dominio_aplicacion"": ""No se puede determinar por falta de documentación"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/tasks_programador,Sí,Sí,"Reset GitGub Programar una Tarea General Desencadenadores Acción para "".bat"" Acciones para un .py Condiciones Configuración Detener la tarea si se ejecuta durante mas de:00:10:00 Historial",README.md,2025-10-28 14:50:18,https://github.com/programmingwithclaudio/tasks_programador,"{""proposito_principal"": ""Automatización de tareas programadas con archivos .bat y .py usando GitHub Actions"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""CI/CD"", ""Automation Tool""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [""GitHub Actions""], ""funcionalidades_clave"": [""Programación de tareas"", ""Ejecución de archivos .bat"", ""Ejecución de archivos .py"", ""Configuración de desencadenadores"", ""Límites de tiempo de ejecución""], ""lenguajes_programacion"": [""Python"", ""Batch Script""], ""tags_adicionales"": [""GitHub"", ""Automation"", ""Scheduled Tasks""]}"
programmingwithclaudio/TestInterviews,Sí,Sí,Pasar las entrevistas Laborales,README.md,2025-11-10 17:39:13,https://github.com/programmingwithclaudio/TestInterviews,"{""proposito_principal"": ""Preparación para entrevistas laborales técnicas"", ""dominio_aplicacion"": ""Educación"", ""tipo_proyecto"": [""Recursos educativos"", ""Ejercicios de programación""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Ejercicios de programación"", ""Preparación para entrevistas""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Educativo"", ""Recursos de estudio"", ""Preparación laboral""]}"
programmingwithclaudio/tipificaciones-etl-db,Sí,Sí,Proyecto TI - ETL AUTOMATIZACIÓN AUTOMATIZACIÓN CCVOX FILES- SQL APLICAR LOS FILTROS DESCARGAR FILES .XLS .XLSX Renombrar el file con %dia%clarocontipificaciones.xls (https://postimg.cc/GThCrJvy) Renombrar el file con %dia%portcontipificaciones.xls (https://postimg.cc/67NFtfXp) Renombrar el file con %dia%clarosintipificaciones.xlsx (https://postimg.cc/G8m7mMSd) Renombrar el file con %dia%clarosintipificaciones.xlsx (https://postimg.cc/G8m7mMSd) ALMACENAMIENTO FILES . ABRIR SI ES NECESARIO Y .,README.md,2024-09-10 23:15:05,https://github.com/programmingwithclaudio/tipificaciones-etl-db,"{""proposito_principal"": ""Automatización de procesos ETL para procesamiento y tipificación de archivos Excel (.xls, .xlsx) con filtros SQL"", ""dominio_aplicacion"": ""Data Processing"", ""tipo_proyecto"": [""ETL Pipeline"", ""Automation Tool""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Procesamiento de archivos Excel"", ""Aplicación de filtros SQL"", ""Renombrado automático de archivos"", ""Almacenamiento de archivos""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""File Processing"", ""Excel Automation"", ""Batch Processing""]}"
programmingwithclaudio/typescript-developer,Sí,Sí,"Typescript sin node Add tsconfig.html.json (sin node) | index.html Typescript con node Add tsconfig.node.json (con node) | Typescript con llms Add tsconfig.node.json (con node) | .env| src/server.ts Docker cortex guia Verificar el MODELO IA en contenedor Detalles de la PC Docker cortex ts-bases/ menloltd cortex-plataforma-de-la-ia Typescript con llms v2 Activar los contenedores del modelo en cortex IA SAAS MERN Add config controllers models routes utils db db - index.ts - app.ts - routes - controllers -- models - utils 1""37 en el video https://www.youtube.com/watch?v=wrHTcjSZQ1Y Make sure you're sending the cookie. In the ""Headers"" tab add from fronted: backend frontend Install and configure styles and libraries google fonts work sans material ui, react icons, react hot toast components (Header) - pages(Chat-Home-Login-NotFound-Signup)-App(Header-Routes)-main(BrowserRouter-ThemeProvider)-rafce 3: 54 https://www.youtube.com/watch?v=bEZI-Kho0w8 https://www.youtube.com/watch?v=RkwbGuL-dzo&list=LL&index=17 obtenr la cookie npm install socket.io contract analysis app backend frontend contract analysis app passport-google-oauth20 contract analysis v2 backend inicio db frontend obtenr la cookie test v2",README.md,2025-03-08 15:16:07,https://github.com/programmingwithclaudio/typescript-developer,"{""proposito_principal"": ""Plataforma de desarrollo con TypeScript para múltiples configuraciones incluyendo IA, MERN stack y análisis de contratos"", ""dominio_aplicacion"": ""Desarrollo de Software"", ""tipo_proyecto"": [""Full Stack Web"", ""API REST"", ""Bot"", ""Dashboard""], ""tecnologias_backend"": [""Node.js"", ""Express.js"", ""Socket.io"", ""Passport-google-oauth20""], ""tecnologias_frontend"": [""React"", ""Material UI"", ""React Icons"", ""React Hot Toast""], ""bases_datos"": [], ""ml_ia"": [""Modelos IA en contenedor Docker"", ""LLMs (Large Language Models)""], ""devops_cloud"": [""Docker"", ""Docker Cortex""], ""funcionalidades_clave"": [""Autenticación con Google OAuth"", ""Chat en tiempo real con Socket.io"", ""Análisis de contratos"", ""Gestión de cookies"", ""Sistema de rutas frontend""], ""lenguajes_programacion"": [""TypeScript"", ""JavaScript""], ""tags_adicionales"": [""Open Source"", ""Template"", ""MERN Stack"", ""SAAS""]}"
programmingwithclaudio/uptask-mern-api,Sí,No,Sin documentación disponible,,2024-12-24 00:09:07,https://github.com/programmingwithclaudio/uptask-mern-api,"{""proposito_principal"": ""No se puede determinar sin documentación disponible"", ""dominio_aplicacion"": ""No se puede determinar sin documentación disponible"", ""tipo_proyecto"": [], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [], ""tags_adicionales"": []}"
programmingwithclaudio/utils-developers,No,Sí,"BASES DE DATOS CON (https://github.com/morhetz/gruvbox) (https://github.com/programmingwithclaudio/dotfiles) (https://opensource.org/licenses/MIT) Mongo Contiene el servicio mongo y interfaz mongo-express en el puerto Nota Login : user:admin- password:admin Independientemente de automatizar por completo el Compose, lo mantengo así por motivos de practicar el flujo de los contenedores y su implementación a producción. Redis Postgres",README.md,2025-04-24 17:01:37,https://github.com/programmingwithclaudio/utils-developers,"{""proposito_principal"": ""Configuración de entorno de desarrollo con bases de datos para practicar flujo de contenedores"", ""dominio_aplicacion"": ""DevOps"", ""tipo_proyecto"": [""Infrastructure as Code"", ""Development Environment""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [""MongoDB"", ""Redis"", ""Postgres""], ""ml_ia"": [], ""devops_cloud"": [""Docker"", ""Docker Compose""], ""funcionalidades_clave"": [""Servicio MongoDB"", ""Interfaz mongo-express"", ""Autenticación básica (user:admin, password:admin)"", ""Gestión de contenedores""], ""lenguajes_programacion"": [], ""tags_adicionales"": [""Development Tools"", ""Container Practice"", ""Open Source""]}"
programmingwithclaudio/verify-operator-num-main,Sí,Sí,"Proyecto de Automatización - Validación del origen de números móviles CAPTCHA Solver: auto hCAPTCHA reCAPTCHA freely python -m pip install dask distributed --upgrade chrome://version https://consulta.portabilidad.pe/?handler=Check /home/crow/.config/google-chrome/Default/Extensions/ Para instalar Google Chrome en Ubuntu, sigue estos pasos. No se instala con directamente, ya que no está disponible en los repositorios predeterminados de Ubuntu, pero puedes hacerlo descargando el archivo desde el sitio oficial. Abre una terminal en Ubuntu. Descarga el archivo de Chrome: Instala el archivo con : Si encuentras errores de dependencias, ejecuta el siguiente comando para corregirlos: Luego de la instalación, puedes iniciar Google Chrome buscando ""Chrome"" en el menú de aplicaciones o ejecutando: ¡Con esto, ya tendrás Chrome instalado en tu sistema suspencion automatica apagado automatico descativar",README.md,2024-10-30 16:38:43,https://github.com/programmingwithclaudio/verify-operator-num-main,"{""proposito_principal"": ""Automatización de validación de origen de números móviles con resolución automática de CAPTCHAs"", ""dominio_aplicacion"": ""Telecomunicaciones"", ""tipo_proyecto"": [""Automation"", ""Scraper"", ""CAPTCHA Solver""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Resolución automática de hCAPTCHA"", ""Resolución automática de reCAPTCHA"", ""Automatización de navegador Chrome"", ""Validación de números móviles"", ""Suspensión automática"", ""Apagado automático""], ""lenguajes_programacion"": [""Python""], ""tags_adicionales"": [""Open Source"", ""Chrome Extension"", ""Ubuntu""]}"
ValerioDev6/frontend-chente,No,Sí,npm install axios,README.md,2025-09-16 13:40:46,https://github.com/ValerioDev6/frontend-chente,"{""proposito_principal"": ""Frontend para aplicación web"", ""dominio_aplicacion"": ""Desarrollo web"", ""tipo_proyecto"": [""Frontend Web""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [], ""lenguajes_programacion"": [""JavaScript""], ""tags_adicionales"": []}"
ValerioDev6/surveyfrontend,Sí,Sí,Surveyfrontend This project was generated with Angular CLI version 17.3.9. Primeros pasos npm instal -g @angular/cli@17.3.9 ng new project-name ng serve (localhost),README.md,2024-10-12 03:44:21,https://github.com/ValerioDev6/surveyfrontend,"{""proposito_principal"": ""Aplicación frontend para encuestas generada con Angular CLI"", ""dominio_aplicacion"": ""Encuestas"", ""tipo_proyecto"": [""Frontend Web""], ""tecnologias_backend"": [], ""tecnologias_frontend"": [""Angular"", ""Angular CLI""], ""bases_datos"": [], ""ml_ia"": [], ""devops_cloud"": [], ""funcionalidades_clave"": [""Desarrollo local con ng serve""], ""lenguajes_programacion"": [""TypeScript""], ""tags_adicionales"": [""Template"", ""Generated Project""]}"
